<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Josh Kasuboski</title><link>https://www.joshkasuboski.com/</link><description>Recent content on Josh Kasuboski</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><copyright>Copyright © 2020, Josh Kasuboski</copyright><lastBuildDate>Sun, 05 Jul 2020 13:58:33 -0500</lastBuildDate><atom:link href="https://www.joshkasuboski.com/index.xml" rel="self" type="application/rss+xml"/><item><title>Server Rack Beginnings</title><link>https://www.joshkasuboski.com/posts/server-rack-1/</link><pubDate>Sun, 05 Jul 2020 13:58:33 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/server-rack-1/</guid><description>&lt;p>My network setup was starting to be a mess of a corner. The natural thing to do is to get a server rack to contain everything 😉&lt;/p>
&lt;h2 id="the-mess">The Mess&lt;/h2>
&lt;figure>&lt;a href="network-mess.jpg">
&lt;img src="network-mess.jpg"
alt="A Jumbled mess of cables" width="300px"/> &lt;/a>
&lt;/figure>
&lt;p>The network corner was already a mess, but adding my &lt;a href="https://www.joshkasuboski.com/posts/home-k8s-raspberry-update/">Raspberry Pi cluster&lt;/a> definitely didn't help.&lt;/p>
&lt;p>You can see the following items if you look closely:&lt;/p>
&lt;ul>
&lt;li>Linksys Router&lt;/li>
&lt;li>5-port switch for the Raspberry Pi cluster&lt;/li>
&lt;li>Raspberry Pi Cluster&lt;/li>
&lt;li>Raspberry Pi 3B+&lt;/li>
&lt;li>Hue Hub&lt;/li>
&lt;/ul>
&lt;p>Not pictured is my ISP modem and the USB power hub for the Pi cluster.&lt;/p>
&lt;h2 id="solution">Solution?&lt;/h2>
&lt;p>I have been following &lt;a href="https://www.reddit.com/r/selfhosted/">r/selfhosted&lt;/a> and &lt;a href="https://www.reddit.com/r/homelab/">r/homelab&lt;/a> for a while. It was perhaps a mistake.&lt;/p>
&lt;p>I now aspire to the setups such as &lt;a href="https://hydn.dev/homelab/">this&lt;/a> and &lt;a href="https://tynick.com/blog/06-06-2019/my-humble-homelab-with-raspberry-pi-rack/">this&lt;/a>.&lt;/p>
&lt;p>The first link has a nice monitor on top and the second has a rackmount Pi enclosure (which I have purchased the bracket for).&lt;/p>
&lt;p>I bought a &lt;a href="https://www.ebay.com/itm/15U-4-Post-Open-Frame-Server-Rack-Enclosure-19-Adjustable-Depth/151584303195?ssPageName=STRK%3AMEBIDX%3AIT&amp;amp;_trksid=p2057872.m2749.l2649">15U open enclosure rack&lt;/a>. It's maybe not the most efficient solution, but definitely feels &amp;hellip; cool.&lt;/p>
&lt;h2 id="setting-up-the-rack">Setting up the Rack&lt;/h2>
&lt;p>The rack arrived with some assembly required. It took me an hour to put it together with the help of a magnetic level.&lt;/p>
&lt;figure>&lt;a href="rack-box.jpg">
&lt;img src="rack-box.jpg"
alt="The server rack some assembly required" width="300px"/> &lt;/a>
&lt;/figure>
&lt;figure>&lt;a href="assembled-rack.jpg">
&lt;img src="assembled-rack.jpg"
alt="Assembled rack" width="300px"/> &lt;/a>
&lt;/figure>
&lt;p>I got everything into a &lt;a href="https://www.ebay.com/itm/Cantilever-Server-Shelf-Vented-Black-Shelves-Rack-Mount-19-1U-12-300mm-Deep/152062041884?ssPageName=STRK%3AMEBIDX%3AIT&amp;amp;_trksid=p2057872.m2749.l2649">shelf&lt;/a> and I'm not convinced it looks all that much better. The messy desk obviously isn't helping 😢.&lt;/p>
&lt;figure>&lt;a href="completed-rack.jpg">
&lt;img src="completed-rack.jpg"
alt="Completed rack" width="300px"/> &lt;/a>
&lt;/figure>
&lt;h2 id="moving-forward">Moving Forward&lt;/h2>
&lt;p>The rack may not have immediately fixed my issues, but I'm planning to either replace or augment my set up to be rackmountable.&lt;/p>
&lt;p>My first thing to rackmount is the Pi Cluster. I'm going to use something like &lt;a href="https://www.kaibader.de/3d-printed-raspberry-pi-rack-mount-with-heat-sink-passive-cooling/">these 3D printed mounts&lt;/a> to put the Pis in the rack.&lt;/p>
&lt;p>I'm also looking at getting a 4U case to transfer my desktop to and switching to a rackmounted switch and router.&lt;/p>
&lt;p>In the meantime the Pi cluster is pretty photogenic.&lt;/p>
&lt;figure>&lt;a href="artsy-pi.jpg">
&lt;img src="artsy-pi.jpg"
alt="Artsy Pi" width="400px"/> &lt;/a>
&lt;/figure></description></item><item><title>Persistent Storage with OpenEBS</title><link>https://www.joshkasuboski.com/posts/openebs-homelab/</link><pubDate>Tue, 23 Jun 2020 18:18:22 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/openebs-homelab/</guid><description>&lt;p>My cluster monitoring prometheus kept falling over because it was running out of disk space. After finally getting annoyed having to restart it, I decided it was time for persistent storage.&lt;/p>
&lt;p>I did have the &lt;a href="https://github.com/rancher/local-path-provisioner">local-path-provisioner&lt;/a> running, but I didn't feel great about using the SD card for general storage.&lt;/p>
&lt;p>I bought 2 &lt;a href="https://www.amazon.com/gp/product/B07T5XGWZY/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&amp;amp;psc=1">USB drives&lt;/a> to add to my workers. In hindsight, I probably should have gotten 3 for better redundancy.&lt;/p>
&lt;p>I decided to go with &lt;a href="https://openebs.io/">OpenEBS&lt;/a>. It works as Container Attached Storage and seemed more lightweight and flexible than other options. They also publish arm64 images which is always a plus.&lt;/p>
&lt;h2 id="prepare-the-cluster">Prepare the cluster&lt;/h2>
&lt;p>I found the OpenEBS docs a little hard to follow, but this is what I ended up doing.&lt;/p>
&lt;ul>
&lt;li>Attach your USB drives to the workers (hopefully yours are labeled)&lt;/li>
&lt;li>install iscsi on every node&lt;/li>
&lt;/ul>
&lt;pre>&lt;code>sudo apt-get update
sudo apt-get install -y open-iscsi
sudo systemctl enable --now iscsid
&lt;/code>&lt;/pre>&lt;h2 id="install-openebs">Install OpenEBS&lt;/h2>
&lt;p>I installed using the helm chart. The only changes with the values file below is basically changing all images to the arm64 version. It seems they don't have great support for a mixed architecture cluster.&lt;/p>
&lt;pre>&lt;code>kubectl create ns openebs
helm repo add openebs https://openebs.github.io/charts
helm install openebs openebs/openebs --namespace openebs --version 1.11.1 -f openebs-values.yaml
&lt;/code>&lt;/pre>&lt;pre>&lt;code>ndm:
image: 'openebs/node-disk-manager-arm64'
ndmOperator:
image: 'openebs/node-disk-operator-arm64'
webhook:
image: 'openebs/admission-server-arm64'
apiserver:
image: 'openebs/m-apiserver-arm64'
localprovisioner:
image: 'openebs/provisioner-localpv-arm64'
snapshotOperator:
controller:
image: 'openebs/snapshot-controller-arm64'
provisioner:
image: 'openebs/snapshot-provisioner-arm64'
provisioner:
image: 'openebs/openebs-k8s-provisioner-arm64'
helper:
image: 'openebs/linux-utils-arm64'
cstor:
pool:
image: 'openebs/cstor-pool-arm64'
poolMgmt:
image: 'openebs/cstor-pool-mgmt-arm64'
target:
image: 'openebs/cstor-istgt-arm64'
volumeMgmt:
image: 'openebs/cstor-volume-mgmt-arm64'
policies:
monitoring:
image: 'openebs/m-exporter-arm64'
analytics:
enabled: false
&lt;/code>&lt;/pre>&lt;p>If all is well you should see the pods in the openebs namespace as healthy. My usb drives automatically showed up as block devices as well.&lt;/p>
&lt;ul>
&lt;li>&lt;code>kubectl get pods -n openebs&lt;/code>&lt;/li>
&lt;li>&lt;code>kubectl get sc&lt;/code>&lt;/li>
&lt;li>&lt;code>kubectl get blockdevice -n openebs&lt;/code>&lt;/li>
&lt;/ul>
&lt;figure>&lt;a href="storage-class-block-devices.png">
&lt;img src="storage-class-block-devices.png"
alt="Block devices detected"/> &lt;/a>
&lt;/figure>
&lt;h2 id="configure-openebs">Configure OpenEBS&lt;/h2>
&lt;p>I differed from the quickstart a little bit here. It was a little confusing to me what I should do with only 2 devices.&lt;/p>
&lt;p>I eventually found this &lt;a href="https://github.com/openebs/openebs-docs/issues/486">issue&lt;/a> talking about configuring volumes with a single replica and made it use 2 pools instead of 1.&lt;/p>
&lt;p>The below yaml will set up a StoragePoolClaim with a maxPools of 2 (which will just use both of my nodes with a drive) and a StorageClass configured to use a single replica. I went with striped because it seemed more flexible and since each node only has 1 disk right now it didn't seem too important.&lt;/p>
&lt;pre>&lt;code>---
apiVersion: openebs.io/v1alpha1
kind: StoragePoolClaim
metadata:
name: cstor-disk
spec:
name: cstor-disk
type: disk
maxPools: 2
poolSpec:
poolType: striped
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: openebs-cstor-1-replica-disk
annotations:
openebs.io/cas-type: cstor
cas.openebs.io/config: |
- name: StoragePoolClaim
value: &amp;quot;cstor-disk&amp;quot;
- name: ReplicaCount
value: &amp;quot;1&amp;quot;
provisioner: openebs.io/provisioner-iscsi
&lt;/code>&lt;/pre>&lt;p>After this is applied, you should be able to see the claims and corresponding pods.&lt;/p>
&lt;figure>&lt;a href="claimed-storage-pool.png">
&lt;img src="claimed-storage-pool.png"
alt="Storage Pool"/> &lt;/a>
&lt;/figure>
&lt;h2 id="persistent-prometheus">Persistent Prometheus&lt;/h2>
&lt;p>I used this StorageClass for my cluster Prometheus. It took awhile for the PersistentVolumeClaim pod to start so the initial mount timed out. After around 5 minutes, it sorted itself out. I was able to delete the Prometheus pod and still retained data.&lt;/p></description></item><item><title>Kubectl List All Resources With Label</title><link>https://www.joshkasuboski.com/posts/list-all-resource-with-label/</link><pubDate>Sun, 24 May 2020 22:19:00 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/list-all-resource-with-label/</guid><description>&lt;p>I recently had to remove a long gone helm release that left behind a bunch of resources. This conflicted when reinstalling, so I needed to find them.&lt;/p>
&lt;h2 id="a-magic-command">A magic command&lt;/h2>
&lt;p>I'm mainly putting this here because it seemed non obvious to me.&lt;/p>
&lt;pre>&lt;code>kubectl api-resources --verbs=list -o name | xargs -n 1 kubectl get -o name -l release=prometheus-operator
&lt;/code>&lt;/pre>&lt;p>That will find all &lt;code>api-resources&lt;/code> that have the label &lt;code>release=prometheus-operator&lt;/code>. I had tried to use &lt;code>kubectl get all -A&lt;/code>, but this seemed to only return built-in types.&lt;/p></description></item><item><title>Building Multiarch Images</title><link>https://www.joshkasuboski.com/posts/build-multiarch-image/</link><pubDate>Sun, 17 May 2020 17:28:55 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/build-multiarch-image/</guid><description>&lt;p>I wanted to use &lt;a href="https://github.com/usefathom/fathom">fathom&lt;/a> on my Raspberry Pi k3s cluster, but they didn't publish a compatible ARM image. Not to be deterred I forked the repo and built my own.&lt;/p>
&lt;h2 id="how-does-one-build-for-multiple-architectures-easily">How does one build for multiple architectures easily&lt;/h2>
&lt;p>Docker has a tool called buildx. It acts as a frontend to buildkit and allows building images for multiple platforms at once.&lt;/p>
&lt;p>I followed &lt;a href="https://www.docker.com/blog/multi-arch-build-and-images-the-simple-way/">this&lt;/a> guide to create a GitHub Actions workflow to build it.&lt;/p>
&lt;h2 id="github-actions-it-up">GitHub Actions it up&lt;/h2>
&lt;p>You can skip ahead and look at my final workflow &lt;a href="https://github.com/kasuboski/fathom/blob/master/.github/workflows/docker.yml">here&lt;/a>.&lt;/p>
&lt;p>There was already a buildx action. The workflow ends up seemingly simple.&lt;/p>
&lt;ul>
&lt;li>Checkout the repo&lt;/li>
&lt;li>Set up buildx&lt;/li>
&lt;li>Login to DockerHub&lt;/li>
&lt;li>Build for linux/amd64,linux/arm/v7,linux/arm64&lt;/li>
&lt;/ul>
&lt;p>That last part includes the platforms I would care about (for now). amd64 is for your run of the mill computer, arm/v7 being what is on my Pi thanks to 32-bit Raspbian, and arm64 being a potential if I switch the OS on my Pi.&lt;/p>
&lt;p>I did have to make a change to the repo other than just adding the workflow. The build step had the &lt;code>GOARCH&lt;/code> variable set to &lt;code>amd64&lt;/code>. This caused the build to always output an &lt;code>amd64&lt;/code> binary, unsurprisingly. I realized this after two 16 minute builds&amp;hellip;&lt;/p>
&lt;p>The workflow works well, but every run so far was between 14mins and 30mins&amp;hellip; not exactly fast.&lt;/p>
&lt;h2 id="moving-forward">Moving Forward&lt;/h2>
&lt;p>There have been a number of images that aren't multi-arch that I want to run. I may look into forking and building those, but it would be nice to have a simpler solution with me not managing it.&lt;/p>
&lt;p>I could contribute to &lt;a href="https://github.com/raspbernetes/multi-arch-images">multi-arch-images&lt;/a> instead. They seem to be doing something similar but just copying the Dockerfiles needed.&lt;/p></description></item><item><title>Homelab Raspberry Pi Kubernetes Cluster Current State</title><link>https://www.joshkasuboski.com/posts/home-k8s-raspberry-update/</link><pubDate>Sun, 10 May 2020 12:20:48 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/home-k8s-raspberry-update/</guid><description>&lt;p>The cluster is alive and well and exposed to the internet 😱&lt;/p>
&lt;p>In a previous &lt;a href="https://www.joshkasuboski.com/posts/home-k8s-raspberry-pi/">post&lt;/a>, I put out my plan for the cluster. I mostly followed along, but did end up getting &lt;a href="https://www.amazon.com/gp/product/B00A128S24">this&lt;/a> switch and &lt;a href="https://www.amazon.com/gp/product/B003L1AET2">these&lt;/a> ethernet cables.&lt;/p>
&lt;p>I ended up with this set-up which is described in more detail below.&lt;/p>
&lt;figure>&lt;a href="traffic-diagram.svg">
&lt;img src="traffic-diagram.png"
alt="Traffic Diagram"/> &lt;/a>
&lt;/figure>
&lt;h2 id="hardware-and-monitoring">Hardware and Monitoring&lt;/h2>
&lt;p>The cluster is set up with Raspbian Lite on 3 Raspberry Pi 4 4gb boards. Raspbian Lite seems to not support 64bit (yet) so I may be changing the operating system. The cluster has the monitoring stack from carlosedp's &lt;a href="https://github.com/carlosedp/cluster-monitoring">repo&lt;/a>. I changed some things (mainly the ingress) in my forked &lt;a href="https://github.com/kasuboski/cluster-monitoring">repo&lt;/a>.&lt;/p>
&lt;p>This deploys some nice Grafana dashboards to see the state of the cluster as seen below. &lt;code>blueberry&lt;/code> is the master node so hovers around 768m cpu cores and 1285Mi memory usage.&lt;/p>
&lt;figure>&lt;a href="cluster-metrics.png">
&lt;img src="cluster-metrics.png"
alt="Cluster Metrics"/> &lt;/a>
&lt;/figure>
&lt;h2 id="ingress">Ingress&lt;/h2>
&lt;p>This dashboard is exposed outside the cluster using &lt;a href="https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#over-a-nodeport-service">ingress-nginx&lt;/a> with NodePorts. These nodeports are load balanced by an haproxy running in a container on my existing Raspberry Pi 3B+. This is a single point of failure for ingress 😰, but should be good enough for now.&lt;/p>
&lt;p>The Ingress is secured using &lt;a href="https://github.com/vouch/vouch-proxy">vouch-proxy&lt;/a> with the ingress-nginx auth &lt;a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/">annotations&lt;/a>. It uses the &lt;a href="https://indieauth.net/">IndieAuth&lt;/a> backend so I can login using this site as an identity.&lt;/p>
&lt;h2 id="expose-to-the-world">Expose to the world&lt;/h2>
&lt;p>The final piece of the puzzle to expose cluster services to the internet is a Linode &lt;a href="https://www.linode.com/products/nanodes/">nanode&lt;/a>. I chose Linode largely because I had a credit&amp;hellip; It's really just providing a public ip address that can forward to my network. I didn't set up port forwarding to my network instead the haproxy Raspberry Pi and the Linode VM are running &lt;a href="https://tailscale.com/">tailscale&lt;/a>.&lt;/p>
&lt;p>Tailscale allows the nanode to connect to my Raspberry Pi with a WireGuard connection through my crazy NAT situation. The nanode is also running an haproxy on ports 80 and 443 on the public ip interface and then proxying that to the tailscale ip of the Raspberry Pi.&lt;/p>
&lt;p>I changed the DNS of &lt;code>*.joshcorp.co&lt;/code> to point to the nanode. Now that &lt;code>*.joshcorp.co&lt;/code> resolves to my cluster on the internet, I can easily use &lt;a href="https://cert-manager.io/docs/">cert-manager&lt;/a> to get a certificate from LetsEncrypt.&lt;/p>
&lt;p>After all of that, I can access &lt;code>grafana.joshcorp.co&lt;/code> outside of my network while authenticating using &lt;a href="https://www.joshkasuboski.com">www.joshkasuboski.com&lt;/a>.&lt;/p>
&lt;h2 id="whats-next">What's Next&lt;/h2>
&lt;p>I'm pretty happy with the current state. My next move will be making sure everything is tracked in git and can be reproduced. From there I can start deploying.&lt;/p></description></item><item><title>Homelab Raspberry Pi Kubernetes Cluster</title><link>https://www.joshkasuboski.com/posts/home-k8s-raspberry-pi/</link><pubDate>Sat, 02 May 2020 16:17:39 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/home-k8s-raspberry-pi/</guid><description>&lt;p>A platform to deploy my self-hosted services onto with almost $0 recurring costs.&lt;/p>
&lt;p>Previously, I was running a few things on a Raspberry Pi 3B+. Mainly, &lt;a href="https://github.com/0xERR0R/blocky">Blocky&lt;/a> as an ad-blocking local dns cache and some adventures with &lt;a href="https://www.home-assistant.io/">Home Assistant&lt;/a>. A Kubernetes cluster will enable me to easily set up new things and have proper monitoring as well.&lt;/p>
&lt;h2 id="hardware-acquisition">Hardware Acquisition&lt;/h2>
&lt;p>I bought 3 &lt;a href="https://www.canakit.com/raspberry-pi-4-4gb.html">Raspberry Pi 4 4GB&lt;/a> boards from Canakit. I also bought a pack of 5 &lt;a href="https://www.amazon.com/gp/product/B07NP96DX5">SD cards&lt;/a>. I got a &lt;a href="https://www.amazon.com/gp/product/B01NAG3V8E">6 port usb charging station&lt;/a> and USB-A to USB-C &lt;a href="https://www.amazon.com/gp/product/B01JRY0VE4">cables&lt;/a> to power them.&lt;/p>
&lt;p>I have 3 spare ethernet ports on my router so I will be using that directly for now. In the future, I may get a switch for them to connect to.&lt;/p>
&lt;h2 id="setup">Setup&lt;/h2>
&lt;p>I'm planning to install Raspian Lite and use &lt;a href="https://github.com/alexellis/k3sup">k3sup&lt;/a> to install k3s. I looked into using &lt;a href="https://github.com/rancher/k3os">k3os&lt;/a> directly, but it seems it's still not the easiest for a Raspberry Pi &lt;a href="https://github.com/rancher/k3os/issues/309">issue&lt;/a>.&lt;/p>
&lt;p>I will try to manage the cluster in a &lt;a href="https://www.weave.works/technologies/gitops/">GitOps&lt;/a> style. As such, I will disable installing the Traefik Ingress Controller and Service Load Balancer by default and instead install them separately (or choose a different option).&lt;/p>
&lt;h2 id="networking">Networking&lt;/h2>
&lt;p>I will start out with the default flannel with vxlan, but am considering the wireguard backend. I would like to be able to access some services in the cluster using something like &lt;a href="https://tailscale.com/">Tailscale&lt;/a> as well.&lt;/p>
&lt;p>This might involve running separate ingress controllers like &lt;a href="https://medium.com/@carlosedp/multiple-traefik-ingresses-with-letsencrypt-https-certificates-on-kubernetes-b590550280cf">here&lt;/a>.&lt;/p>
&lt;h2 id="monitoring">Monitoring&lt;/h2>
&lt;p>I'm going to deploy the standard &lt;code>kube-prometheus&lt;/code> set up with the Prometheus Operator using this &lt;a href="https://github.com/carlosedp/cluster-monitoring">repo&lt;/a>. It has k3s specific settings.&lt;/p></description></item><item><title>Deploying this site with GitHub Actions</title><link>https://www.joshkasuboski.com/posts/deploy-site-github-actions/</link><pubDate>Sun, 05 Apr 2020 16:18:12 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/deploy-site-github-actions/</guid><description>&lt;p>&lt;a href="https://pages.github.com/">GitHub pages&lt;/a> is a free static hosting provider that unsurprisingly works well with a git workflow. It enables &lt;code>git push&lt;/code> to deploy type workflows. This site itself is a static site built with &lt;a href="https://gohugo.io/">hugo&lt;/a> and deployed to GitHub Pages using GitHub Actions.&lt;/p>
&lt;h2 id="github-pages-repo">GitHub Pages Repo&lt;/h2>
&lt;p>GitHub Pages expects that you have a repo named something like &lt;code>user.github.io&lt;/code> that has static files you want deployed. However, if your site requires any sort of building this doesn't really work for you (at least if you want to automate the build step).&lt;/p>
&lt;p>One way around this is to treat your GitHub Pages Repo as the output for your build. You create a separate repo that just has the built static files. You can see the repo for this site at &lt;a href="https://github.com/kasuboski/kasuboski.github.io">kasuboski/kasuboski.github.io&lt;/a>.&lt;/p>
&lt;p>You'll notice it just has static html, css, etc. I write the content for this site in markdown however, using Hugo.&lt;/p>
&lt;h2 id="code-repo">Code Repo&lt;/h2>
&lt;p>The actual content is stored in a separate repo that I'm referring to as the code repo. The repo for this site is &lt;a href="https://github.com/kasuboski/personal-site">kasuboski/personal-site&lt;/a>. This repo should look a little more familiar to anyone who has used Hugo.&lt;/p>
&lt;p>The content is under &lt;code>content/posts&lt;/code> and is written in markdown. This is the repo that I actually modify and have checked out locally. If I wanted to create a new release of it manually I would build the site and push it to the GitHub Pages Repo with the commands below.&lt;/p>
&lt;pre>&lt;code>hugo --minify
cp ./public ../kasuboski.github.io/
cd ../kasuboski.github.io
git commit -am &amp;quot;cool new post&amp;quot;
git push origin master
&lt;/code>&lt;/pre>&lt;h2 id="automatic-deploy-with-github-actions">Automatic Deploy with GitHub Actions&lt;/h2>
&lt;p>I don't really want to mess with multiple repositories locally, especially when one of them is essentially machine generated. GitHub Actions can build the site with Hugo and then push it to the GitHub Pages Repo for me.&lt;/p>
&lt;p>The workflow file for this can be found &lt;a href="https://github.com/kasuboski/personal-site/blob/master/.github/workflows/gh-pages.yaml">here&lt;/a> if you just want to copy it.&lt;/p>
&lt;p>The basic steps are: check out the code repo, build the site with hugo, push the files to the GitHub Pages Repo, and notify me of the status using &lt;a href="https://pushover.net">Pushover&lt;/a>.&lt;/p>
&lt;p>It relies heavily on already available actions. The only prerequisite setup is to make a deploy key for the GitHub Pages Repo and to register an app with Pushover.&lt;/p>
&lt;p>You can find a walkthrough of creating a deploy key &lt;a href="https://github.com/peaceiris/actions-gh-pages#%EF%B8%8F-create-ssh-deploy-key">here&lt;/a>.&lt;/p>
&lt;p>Configuring a Pushover app is &lt;a href="https://pushover.net/apps/build">here&lt;/a>. You'll need the app token and user token. I added both as secrets on the Code Repo.&lt;/p>
&lt;p>Now anytime I push a change to the code repo, GitHub Actions will generate the files, update the GitHub Pages repo and send me a push notification with the status.&lt;/p></description></item><item><title>Weather Data after Dark Sky Shutdown</title><link>https://www.joshkasuboski.com/posts/dark-sky-shutdown/</link><pubDate>Sun, 05 Apr 2020 15:03:48 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/dark-sky-shutdown/</guid><description>&lt;p>Dark Sky is discontinuing their API and Android app after &lt;a href="https://blog.darksky.net/dark-sky-has-a-new-home/">joining Apple&lt;/a>. It was my favorite weather app, both from a data and UX perspective. They apparently aggregate from many different sources to have the best predictions and world wide coverage. I'm not sure I actually need all of that and am curious to make my own solution.&lt;/p>
&lt;h2 id="making-my-own-app">Making my own app&lt;/h2>
&lt;p>This seems like a great opportunity to learn &lt;a href="https://flutter.dev/">Flutter&lt;/a>. It's been a while since I've done mobile development. I did both native Android and iOS as well as React Native. I'm not too keen to be building two of everything and React Native was a tooling nightmare for me.&lt;/p>
&lt;p>Flutter seems like a great solution especially with its component driven concepts. However, in order to make a weather app you actually need the forecast data.&lt;/p>
&lt;h2 id="getting-the-forecast">Getting the forecast&lt;/h2>
&lt;p>I looked at using the National Weather Service API at &lt;a href="https://www.weather.gov/documentation/services-web-api">weather.gov&lt;/a>, but it seems it only has data for a specific Kansas station at the moment. This kind of kills the project for the time being, but I hope to find the info elsewhere.&lt;/p>
&lt;p>I saw &lt;a href="https://aaronparecki.com/">Aaron Parecki&lt;/a> seems to use &lt;a href="https://www.wunderground.com/">Wunderground&lt;/a> to get current weather for &lt;a href="https://aaronparecki.com/2018/07/03/7/">his posts&lt;/a>. I'll have to see what their API is like and if there are agreeable terms.&lt;/p>
&lt;p>I believe the National Weather Service also publishes the data in a not so convenient format. I could look into downloading that periodically and exposing it myself. Maybe I could use my phone location to only download the info that I'm most likely to care about.&lt;/p></description></item><item><title>Exporting Google Saved Places</title><link>https://www.joshkasuboski.com/posts/export-google-saved-places/</link><pubDate>Sat, 25 Jan 2020 12:40:00 -0600</pubDate><guid>https://www.joshkasuboski.com/posts/export-google-saved-places/</guid><description>&lt;p>I've made use of Google Saved Places for a while now. I have the standard ⭐ &amp;ldquo;Starred&amp;rdquo;, 🚩 &amp;ldquo;Want to go&amp;rdquo;, and ❤ &amp;ldquo;Favorites&amp;rdquo; lists as well as some specific to cities. Saved places is really convenient when you're deciding where to go. My want to go list helps me remember the places I want to try and the favorites list often helps me find a place if I can't quite remember the name of it.&lt;/p>
&lt;p>It's not all rosy though&amp;hellip;when searching for anything in Google Maps it inexplicably doesn't show you if the places returned are in any of your lists. They also recently made the interface better for adding places to multiple lists, but you can't search your places by city let alone type (say coffee shops I love). This could be accomplished by having many lists, however I'm not looking to manually manage a growing list of lists.&lt;/p>
&lt;h2 id="what-to-do-about-it">What to do about it&lt;/h2>
&lt;p>What I really want is a list of places with their name, address, and general categories such as restaurant or coffee shop. Then I can tag those places with things like starred, favorite, want to go, etc.&lt;/p>
&lt;p>Once I have that, I will be able to search my places by location, tag, and/or category. I'll want to be able to see them on a map similarly to how you can navigate around a city in Google Maps and see your saved places.&lt;/p>
&lt;p>I'm going to start with all of my data from Google Saved Places and then add the info I want. Since I don't plan to stop using the Google Saved Places just yet, I'll need to be able to run the import multiple times. I'll make a &lt;a href="https://en.wikipedia.org/wiki/Command-line_interface">cli&lt;/a> to help with importing the data.&lt;/p>
&lt;h2 id="getting-the-data">Getting the data&lt;/h2>
&lt;p>There doesn't seem to be an API to access your saved places. I had to resort to using &lt;a href="https://takeout.google.com">Google Takeout&lt;/a>, which if you haven't used before is quite interesting to see everything Google knows about you.&lt;/p>
&lt;p>Getting the data I wanted out of Takeout was a bit of trial and error. There are two options for Maps data as seen below, Maps and Maps (your places). Maps (your places) includes lists that aren't the starred list. Apparently, Google treats starred separately. I'm guessing this is because historically starred was the only option and was called saved. Another fun fact is that those starred places also show up at &lt;a href="https://www.google.com/bookmarks">Google Bookmarks&lt;/a>.&lt;/p>
&lt;p>&lt;img src="https://www.joshkasuboski.com/img/maps-export-products.png" alt="">&lt;/p>
&lt;p>If you only want your starred places from Maps you'll want to select &amp;ldquo;All Maps data included&amp;rdquo; and select only &amp;ldquo;My labeled places&amp;rdquo;. Running the export with the two maps options will get you a link to download a &lt;code>.zip&lt;/code> file. Once extracted, you'll have a variety of folders. The important files are &lt;code>Saved Places.json&lt;/code> and &lt;code>*.csv&lt;/code>.&lt;/p>
&lt;p>The Saved Places file will have your starred list in &lt;a href="https://en.wikipedia.org/wiki/GeoJSON">GeoJSON&lt;/a> format. This format provides a fair amount of info like place name, location, and general categories. The csv files are a lot less useful by themselves. For me, it was just place name and a URL. However, the URL didn't seem to be parsable so this data wasn't super useful. Still, I was able to get all of the place names that I had saved.&lt;/p>
&lt;h2 id="enhance">Enhance!&lt;/h2>
&lt;p>I want the name, address, and categories of my places. The geojson file has everything, but the csv files only provide a name. Enter the &lt;a href="https://developers.google.com/places/web-service/intro">Google Places API&lt;/a> 💥. This API let's you lookup place data from Google, including all of the fields I want (besides user defined tags).&lt;/p>
&lt;p>Unfortunately, neither of the formats I exported from Google gave me the &lt;a href="https://developers.google.com/places/web-service/place-id">Place ID&lt;/a>, which would make it easy to find a specific place. The json file gave me a place name and address though so it's fairly simple to do a &lt;a href="https://developers.google.com/places/web-service/search">Place Search&lt;/a> using the name and address as the input. The csv files providing only the place name and obscure URL is a little more difficult.&lt;/p>
&lt;p>I ended up scraping the URL that was included for a string that looked like a Place ID. This definitely isn't perfect&amp;hellip;especially since I only look for one of at least two possible formats, but it seemed to work for my data.&lt;/p>
&lt;h2 id="next-steps">Next steps&lt;/h2>
&lt;p>So, I was able to get my saved places out of Google and tag them with the list they were on. I'm pretty much at the exact point I was at with Google Places minus the Google Maps integration. 😒&lt;/p>
&lt;p>I want to build up an interface to add more tags easily. I also need to build the search and visualization aspects for discovering my places.&lt;/p>
&lt;p>I'd also like to set up the ability to post this info to my site using &lt;a href="https://indieweb.org/Micropub">micropub&lt;/a>, but that presumes I have micropub set up here at all.&lt;/p>
&lt;p>The code I used to parse and save my data is on GitHub. It's a &lt;a href="https://en.wikipedia.org/wiki/Command-line_interface">cli&lt;/a> called &lt;a href="https://github.com/kasuboski/neptune">neptune&lt;/a>. At this time, it let's you import and tag places and it will export each place to a json file in a folder.&lt;/p></description></item><item><title>Replacing Feedly with Microsub?</title><link>https://www.joshkasuboski.com/posts/replacing-feedly/</link><pubDate>Wed, 15 Jan 2020 18:48:10 -0600</pubDate><guid>https://www.joshkasuboski.com/posts/replacing-feedly/</guid><description>&lt;p>I use &lt;a href="https://feedly.com">Feedly&lt;/a> to manage my content subscriptions, which include a number of bigger sites and personal blogs. Feedly is nice, but I would like to be able to save and use the data from there in other ways. So, I've been looking for an open source setup that I can tweak.&lt;/p>
&lt;p>I've been trying to utilize &lt;a href="https://indieweb.org">IndieWeb&lt;/a> pieces more and more. Their &lt;a href="https://indieweb.org/why">why&lt;/a> really resonates with some of my frustrations with the current web (mainly auth and data ownership/portability). This site supports &lt;a href="https://indieauth.com/">IndieAuth&lt;/a> so I can login at supporting websites by giving my URL. The posts and contact card are also marked up with &lt;a href="http://microformats.org/">microformat&lt;/a> to be parseable.&lt;/p>
&lt;h2 id="experimenting-with-aperture-and-monocle">Experimenting with Aperture and Monocle&lt;/h2>
&lt;p>I decided to use &lt;a href="https://aperture.p3k.io/">Aperture&lt;/a> for now as my &lt;a href="https://indieweb.org/Microsub">microsub&lt;/a> server. A microsub server is responsible for fetching the content you subscribe to and making it available in a common format for a microsub reader.&lt;/p>
&lt;p>I was able to login and subscribe to &lt;a href="https://aaronparecki.com/">Aaron Parecki's&lt;/a> personal site, which immediately loaded some 1600 entries for me. After adding a &lt;code>rel=microsub&lt;/code> link to my homepage, I was able to log into &lt;a href="https://monocle.p3k.io/">Monocle&lt;/a> and view that feed in my home channel. The default view for Monocle seems to be to show everything. There is an option to only show unread, but it's not what you get by default.&lt;/p>
&lt;h2 id="moving-forward">Moving forward&lt;/h2>
&lt;p>Monocle doesn't quite fit how I want to view updates, but it did help me understand the concepts better. The free hosted Aperture only saves your data for 7 days so I probably need to either host it myself or find a different microsub server.&lt;/p>
&lt;p>I've been looking at &lt;a href="https://github.com/pstuifzand/ekster">ekster&lt;/a>. I like that it's a go binary and comes with a CLI. It has the option of importing an opml feed, which Feedly would export. It seems all of your channels and feeds are stored in a config file (that you can generate with the opml import) and redis is really meant to be a cache.&lt;/p>
&lt;p>It does seem to support the &lt;a href="https://indieweb.org/Microsub-spec#Following">follow action&lt;/a> however and it doesn't look like that updates the file. In the future, I'll probably just try to run it and see what happens.&lt;/p>
&lt;p>Ekster also has a reader associated with it, but there are a number of &lt;a href="https://indieweb.org/Microsub#Clients">others&lt;/a> to try including mobile apps.&lt;/p></description></item><item><title>Now</title><link>https://www.joshkasuboski.com/now/</link><pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.joshkasuboski.com/now/</guid><description>&lt;blockquote>
&lt;p>A now page inspired by &lt;a href="https://sivers.org/nowff">Derek Sivers&amp;rsquo;&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>Currently working on developer productivity tools at PNC.&lt;/p>
&lt;p>In my free time, I try to continue learning about things.&lt;/p>
&lt;p>My current interests are:&lt;/p>
&lt;ul>
&lt;li>Decentralized Services&lt;/li>
&lt;li>Self Hosting&lt;/li>
&lt;li>Quantified Self&lt;/li>
&lt;li>&lt;a href="https://indieweb.org/">IndieWeb&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>My Tech Setup is:&lt;/p>
&lt;ul>
&lt;li>Pixel 3a&lt;/li>
&lt;li>2012 Macbook Pro Retina&lt;/li>
&lt;li>Custom Desktop (Ryzen 5 1600, 16gb, 1060)&lt;/li>
&lt;/ul></description></item><item><title>Cheap Managed Kubernetes with Terraform</title><link>https://www.joshkasuboski.com/posts/cheap-managed-kube/</link><pubDate>Thu, 18 Apr 2019 14:15:59 -0600</pubDate><guid>https://www.joshkasuboski.com/posts/cheap-managed-kube/</guid><description>&lt;p>Kubernetes is a great way to deploy your services in a scalable and reliable way. However, it's a pretty complex system to manage yourself. Thankfully, cloud providers are offering managed versions where you only pay for the worker nodes.&lt;/p>
&lt;p>We'll use &lt;a href="https://cloud.google.com/kubernetes-engine/">GKE&lt;/a>, Google's managed kubernetes offering, to deploy a cluster so we can test out kubernetes.&lt;/p>
&lt;p>We'll use &lt;a href="https://www.terraform.io/">Terraform&lt;/a> to make sure we have a repeatable deployment process.&lt;/p>
&lt;p>If you just want to skip to the code it's on &lt;a href="https://github.com/kasuboski/cheap-managed-kubernetes">GitHub&lt;/a>.&lt;/p>
&lt;h2 id="what-well-do">What we'll do&lt;/h2>
&lt;p>The resources we'll deploy use the Google Cloud &lt;a href="https://cloud.google.com/free/">free-tier&lt;/a> extensively. If you leave it running, it should cost a little over $5 a month.&lt;/p>
&lt;p>If you're not familiar with Terraform or haven't used the Google Provider, you can get started &lt;a href="https://www.terraform.io/docs/providers/google/getting_started.html">here&lt;/a>. All of the resources it deploys will be in the free tier.&lt;/p>
&lt;p>Terraform has a concept of remote backends which allow you to save the state of your deployments (not just on your machine). This is especially helpful if you have multiple team members.&lt;/p>
&lt;p>Since we're already using Google Cloud we can use Google Cloud Storage to house our state. After changing some defaults we can run a few commands and have our cluster running.&lt;/p>
&lt;h2 id="actually-do-it">Actually do it&lt;/h2>
&lt;ul>
&lt;li>Create a Google Cloud Storage Bucket following these &lt;a href="https://cloud.google.com/storage/docs/creating-buckets">instructions&lt;/a>&lt;/li>
&lt;li>Clone the cheap-managed-kubernetes &lt;a href="https://github.com/kasuboski/cheap-managed-kubernetes">repo&lt;/a>&lt;/li>
&lt;li>Modify &lt;code>terraform.tfvars.example&lt;/code> with your gcp project and rename to &lt;code>terraform.tfvars&lt;/code>&lt;/li>
&lt;li>Modify &lt;code>backend.hcl.example&lt;/code> with the gcs bucket you created above and rename to &lt;code>backend.hcl.example&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>You should now be set up to deploy with Terraform. We'll initialize Terraform with our remote backend and run a plan. This plan will output what will be created (or destroyed). You can verify the output of the plan is correct and then run the apply.&lt;/p>
&lt;ul>
&lt;li>&lt;code>terraform init -backend-config=backend.hcl&lt;/code>&lt;/li>
&lt;li>&lt;code>terraform plan&lt;/code> This should say it will create a cluster and node pool.&lt;/li>
&lt;li>&lt;code>terraform apply&lt;/code> This will actually create the cluster and node pool.&lt;/li>
&lt;li>When you're done &lt;code>terraform destroy&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="using-your-cluster">Using your cluster&lt;/h2>
&lt;p>The output of the apply will give you the info you need to create a &lt;code>kubeconfig&lt;/code> to be able to connect to your cluster. Since we're using GKE though, I find it easier to just use the &lt;code>gcloud&lt;/code> command that will set your &lt;code>kubeconfig&lt;/code> for you.&lt;/p>
&lt;p>It should look something like &lt;code>gcloud container clusters get-credentials my-poor-gke-cluster&lt;/code> where &lt;code>my-poor-gke-cluster&lt;/code> is the name of the cluster resource in &lt;code>main.tf&lt;/code>&lt;/p>
&lt;p>Once you have your &lt;code>kubeconfig&lt;/code> set up, you can access your cluster like you normally would. Maybe try running &lt;code>kubectl get pods --all-namespaces&lt;/code>. You should see the pods that make up &lt;code>kube-system&lt;/code>.&lt;/p></description></item><item><title>About</title><link>https://www.joshkasuboski.com/about/</link><pubDate>Tue, 09 Apr 2019 00:00:00 +0000</pubDate><guid>https://www.joshkasuboski.com/about/</guid><description>&lt;h1 id="hi-there">Hi there&lt;/h1>
&lt;p>My name is Josh and I'm a software engineer. I work on all manner of things across mobile, web, and backend apps.&lt;/p>
&lt;p>Right now, my main interest is improving the developer experience and making development more accessible to take code from laptop to production.&lt;/p>
&lt;p>This involves making pipelines, eliminating boilerplate, and sending messages to make sure it's painless to deploy your code.&lt;/p>
&lt;p>You can find me on &lt;a href="https://github.com/kasuboski">Github&lt;/a> or &lt;a href="https://www.linkedin.com/in/joshkasuboski/">LinkedIn&lt;/a>&lt;/p>
&lt;p>Here is a &lt;a href="https://www.joshkasuboski.com/resume.html">resume&lt;/a>&lt;/p></description></item></channel></rss>