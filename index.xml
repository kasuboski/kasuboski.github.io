<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Josh Kasuboski</title><link>https://www.joshkasuboski.com/</link><description>Recent content on Josh Kasuboski</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><copyright>Copyright Â© 2020, Josh Kasuboski</copyright><lastBuildDate>Sun, 08 May 2022 15:34:19 -0500</lastBuildDate><atom:link href="https://www.joshkasuboski.com/index.xml" rel="self" type="application/rss+xml"/><item><title>Running a Buildkit ARM Builder</title><link>https://www.joshkasuboski.com/posts/buildkit-builder/</link><pubDate>Sun, 08 May 2022 15:34:19 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/buildkit-builder/</guid><description>&lt;p>I was sick of my hour long ARM docker builds. A 15x speedup using existing infrastructure isn't bad.&lt;/p>
&lt;h2 id="the-problem">The Problem&lt;/h2>
&lt;p>I build my feedreader for ARM in Github Actions. The workflow builds a multi-arch docker image on push. The x86 build was pretty quick, but the ARM build was using qemu which made it take around an hour. QEMU certainly didn't help, but the Github Actions runners aren't exactly the biggest machines at 2 vcpu and 7GB ram.&lt;/p>
&lt;p>My build was using the &lt;a href="https://github.com/docker/setup-buildx-action">docker buildx&lt;/a> action. This makes the build use the newish buildkit backend for docker, but it's still running on the actions runner. I wanted to see if I could run my own buildkit backend. There was the option to connect it with a remote docker endpoint or a kubernetes cluster. Neither of which are really that appealing to me, although exposing the docker daemon over Tailscale could be fun.&lt;/p>
&lt;h2 id="the-solution">The Solution&lt;/h2>
&lt;p>Right as I was looking to run my own &lt;code>buildkitd&lt;/code>, &lt;code>buildx&lt;/code> had a PR merged that would enable a remote builder driver. This lets you run &lt;code>buildkitd&lt;/code> somewhere and expose it over tcp. My &lt;a href="https://www.joshkasuboski.com/posts/multi-region-k3s/">kubernetes cluster&lt;/a> has a free ARM node from Oracle that is pretty big (4 x 24GB). It's usually nowhere near fully utilized.&lt;/p>
&lt;p>Running a builder on it seemed like a great way to use the excess resources. Combined with &lt;code>tailscale&lt;/code> and the recommended mTLS auth I could have a rather secure build runner on my existing infrastructure.&lt;/p>
&lt;h2 id="setting-it-up">Setting it up&lt;/h2>
&lt;p>The &lt;a href="https://github.com/moby/buildkit#expose-buildkit-as-a-tcp-service">buildkit&lt;/a> repo as instructions for running it over TCP. There is also an &lt;a href="https://github.com/moby/buildkit/tree/master/examples/kubernetes#deployment--service">example&lt;/a> that shows how to run it in kubernetes with a deployment. I chose the deployment and service option vs a statefulset with consistent hashing because I was planning to use registry caching anyway and don't have immediate plans for many different builds to use this.&lt;/p>
&lt;p>I decided to expose it with Tailscale using the same &lt;a href="https://www.joshkasuboski.com/posts/tailscale-connect-kubernetes-pods/">process&lt;/a> I had previously used for my feedreader. This means connecting to it requires you be on my tailnet (authenticated with Tailscale).&lt;/p>
&lt;p>In addition to requiring you be authenticated with Tailscale, the doc still recommends you use mTLS because the steps being built in the builder could potentially access the daemon as well. The example has a script to set up the certs for you, but I wanted to use the step cli from &lt;a href="https://smallstep.com/">Smallstep&lt;/a>. It's still very simple, but I could control exactly what is set up.&lt;/p>
&lt;h3 id="creating-certificates">Creating Certificates&lt;/h3>
&lt;p>The first step to run &lt;code>buildkitd&lt;/code> was to create the certificates it wants. I decided to make a Root CA for this along with an Intermediate CA and then server and client certificates. I didn't spend too long debating this and just followed a Smallstep guide&amp;hellip;&lt;/p>
&lt;p>Creating the CA&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">step certificate create --profile root-ca &lt;span style="color:#e6db74">&amp;#34;Buildkit Root CA&amp;#34;&lt;/span> root_ca.crt root_ca.key
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Creating the Intermediate CA&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">step certificate create &lt;span style="color:#e6db74">&amp;#34;Buildkit Intermediate CA 1&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> intermediate_ca.crt intermediate_ca.key &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> --profile intermediate-ca --ca ./root_ca.crt --ca-key ./root_ca.key
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Creating the server cert&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">step certificate create buildkitd --san buildkitd --san localhost --san 127.0.0.1 buildkitd.crt buildkitd.key &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> --profile leaf --not-after&lt;span style="color:#f92672">=&lt;/span>8760h &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> --ca ./intermediate_ca.crt --ca-key ./intermediate_ca.key --bundle --no-password --insecure
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Creating the client cert&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">step certificate create client client.crt client.key &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> --profile leaf --not-after&lt;span style="color:#f92672">=&lt;/span>8760h &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> --ca ./intermediate_ca.crt --ca-key ./intermediate_ca.key --bundle --no-password --insecure
&lt;/code>&lt;/pre>&lt;/div>&lt;p>You'll notice the server has a &lt;code>buildkitd&lt;/code> san, which is how I'll access it over Tailscale. The &lt;code>local&lt;/code> ones were for testing while port forwarding to the cluster.&lt;/p>
&lt;h3 id="running-the-server">Running the Server&lt;/h3>
&lt;p>You can find the example kubernetes yaml &lt;a href="https://github.com/moby/buildkit/blob/master/examples/kubernetes/deployment%2Bservice.rootless.yaml">here&lt;/a>. It expects a kubernete secret with &lt;code>ca.pem&lt;/code> and &lt;code>key.pem&lt;/code> keys. You can generate that from below.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl create secret generic buildkit-daemon-certs --from-file&lt;span style="color:#f92672">=&lt;/span>key.pem&lt;span style="color:#f92672">=&lt;/span>buildkitd.key --from-file&lt;span style="color:#f92672">=&lt;/span>ca.pem&lt;span style="color:#f92672">=&lt;/span>root_ca.crt --dry-run&lt;span style="color:#f92672">=&lt;/span>client -oyaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>My actual deployment can be found in &lt;a href="https://github.com/kasuboski/k8s-gitops/tree/main/builder/buildkit">kasuboski/k8s-gitops&lt;/a>. It includes the &lt;a href="https://github.com/kasuboski/tailscale-proxy">tailscale-proxy&lt;/a> as well as a &lt;code>nodeSelector&lt;/code> to make sure it schedules on the ARM node. It requests 1cpu and 512Mi with the limit set to 3.5cpu and 3Gi. It ends up having more cpu than Github Actions and isn't emulated. The memory is less, but hasn't been an issue.&lt;/p>
&lt;p>Once the server is running, it will be available in tailscale at &lt;code>buildkitd&lt;/code> since the proxy uses the deployment name.&lt;/p>
&lt;h3 id="connecting-as-a-client">Connecting as a Client&lt;/h3>
&lt;p>The client needs to have access over tailscale and a client cert. The easiest way is to use &lt;code>buildctl&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">buildctl --addr &lt;span style="color:#e6db74">&amp;#39;tcp://buildkitd:1234&amp;#39;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> --tlscacert root_ca.crt &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> --tlscert client.crt &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> --tlskey client.key &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> build --frontend dockerfile.v0 --local context&lt;span style="color:#f92672">=&lt;/span>. --local dockerfile&lt;span style="color:#f92672">=&lt;/span>.
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Building on multiple platforms with different builders requires &lt;code>docker buildx&lt;/code>. The remote driver is on &lt;code>master&lt;/code>, but isn't in a release yet. You can build buildx yourself to get access to that feature, but I only used it from Github Actions.&lt;/p>
&lt;p>The &lt;a href="https://github.com/docker/setup-buildx-action">setup buildx&lt;/a> has the option to build buildx from a specific commit.&lt;/p>
&lt;h3 id="running-in-github-actions">Running in Github Actions&lt;/h3>
&lt;p>If you want to skip to the workflow it's at &lt;a href="https://github.com/kasuboski/feedreader/blob/696debe2da1d26f1e4047806ff5e1f5ca5fbe347/.github/workflows/ci.yaml">kasuboski/feedreader&lt;/a>.&lt;/p>
&lt;p>The workflow will need secrets for Tailscale and the certificates. I use &lt;a href="https://doppler.com/join?invite=390F66AC">Doppler&lt;/a> &lt;em>referral link&lt;/em> to manage the secrets. It synced super fast and has a nicer interface than doing it per repo in Github imo.&lt;/p>
&lt;p>Tailscale has a Github Action that will install and set it up given an auth key. They support ephemeral auth keys so you want have a bunch of leftover machines in their system. Once installed, your workflow will have access to your tailneta and can reach &lt;code>buildkitd&lt;/code>. It's worth noting DNS also magically works thanks to &lt;a href="https://tailscale.com/kb/1081/magicdns/">Magic DNS&lt;/a>. Connecting to a kubernetes pod with a nice name and no other network setup is life changing.&lt;/p>
&lt;p>I had problems using the remote buildx driver with a different builder type. I ended up just running another &lt;code>buildkitd&lt;/code> on the actions runner. In the future, I'd like to run a x86 builder on one of my nodes.&lt;/p>
&lt;p>That's setup following inspiration from the buildx tests. This builder doesn't have mTLS setup, but I guess I'm fine for now since it's an ephemeral runner on Github's infrastructure ðŸ¤·â€â™‚ï¸.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">docker run -d --name buildkitd --privileged -p 1234:1234 moby/buildkit:buildx-stable-1 --addr tcp://0.0.0.0:1234
docker buildx create --name gh-builder --driver remote --use tcp://0.0.0.0:1234
docker buildx inspect --bootstrap
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Adding my arm runner is done after as below. The certs have already been written to disk from the Github secrets.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">docker buildx create --append --name gh-builder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> --node arm &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> --driver remote &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> --driver-opt key&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$GITHUB_WORKSPACE&lt;span style="color:#e6db74">/key.pem&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> --driver-opt cert&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$GITHUB_WORKSPACE&lt;span style="color:#e6db74">/client_cert.pem&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> --driver-opt cacert&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$GITHUB_WORKSPACE&lt;span style="color:#e6db74">/ca_cert.pem&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;span style="color:#ae81ff">&lt;/span> tcp://buildkitd:1234
docker buildx ls
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>docker buildx ls&lt;/code> output should then show a &lt;code>gh-builder&lt;/code> with two nodes, one supporting &lt;code>amd64&lt;/code> and the other &lt;code>arm&lt;/code>.
&lt;figure>&lt;a href="buildx-ls.png">
&lt;img src="buildx-ls.png"
alt="Docker Buildx List"/> &lt;/a>
&lt;/figure>
&lt;/p>
&lt;p>After setting up the builder, the workflow went from an hour to under four minutes.&lt;/p>
&lt;h2 id="next-steps">Next Steps&lt;/h2>
&lt;p>Building on my excess capacity has been great and I want to add an x86 node as well. I could have run my own Github Runners, but that seems much more intense. All of my repos are public as well, so I figure I might as well use the free actions minutes.&lt;/p>
&lt;p>I do want to potentially make a service that will just give you a remote buildkit builder on demand. It's particularly helpful for ARM builds since those can be slow in emulation.&lt;/p>
&lt;p>I also looked into the cross compilation options, but just getting a native builder seemed easier and more flexible. Your &lt;code>Dockerfile&lt;/code> still has to not download a specific architecture explicitly, but otherwise most should be able to build multi-arch with this setup.&lt;/p></description></item><item><title>Connect to Kubernetes Pods with Tailscale</title><link>https://www.joshkasuboski.com/posts/tailscale-connect-kubernetes-pods/</link><pubDate>Sun, 03 Apr 2022 13:52:19 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/tailscale-connect-kubernetes-pods/</guid><description>&lt;p>I wasn't ready to add auth to my new feedreader. So I built a moat with Tailscale.&lt;/p>
&lt;h2 id="what-does-that-_mean_">What does that &lt;em>mean&lt;/em>?&lt;/h2>
&lt;p>I added a kubernetes pod to my tailnet so it's accessible from anywhere that can route to Tailscale nodes. It also gets a nice domain name so &lt;code>http://feedreader&lt;/code> works.&lt;/p>
&lt;p>Tailscale actually has a &lt;a href="https://tailscale.com/blog/kubecon-21/">blog post&lt;/a> and &lt;a href="https://github.com/tailscale/tailscale/tree/main/docs/k8s">example&lt;/a> for how to set this up. I wanted it to be mildly different so modified their run script. They also don't publish their example image so I needed to build one.&lt;/p>
&lt;p>You can see my version at &lt;a href="https://github.com/kasuboski/tailscale-proxy">kasuboski/tailscale-proxy&lt;/a>. The main differences are it takes a &lt;code>HOSTNAME&lt;/code> and &lt;code>DEST_PORT&lt;/code> parameters.&lt;/p>
&lt;p>The &lt;code>HOSTNAME&lt;/code> is so you can set the name the node will show up as. This was important for kubernetes because I don't want the generated name of the pod to be how I access it. &lt;code>DEST_PORT&lt;/code> is so you can have it forward to a different port. This allows you to run your app on &lt;code>8080&lt;/code>, but the &lt;code>tailscale-proxy&lt;/code> will route any port to &lt;code>8080&lt;/code> meaning you can hit it on &lt;code>80&lt;/code> in your browser. This is required to not have &lt;code>:8080&lt;/code> ugliness after your URL.&lt;/p>
&lt;h2 id="but-why">But Why?&lt;/h2>
&lt;p>I've been making a &lt;a href="https://github.com/kasuboski/feedreader">feedreader&lt;/a> to replace my running &lt;a href="https://miniflux.app/">miniflux&lt;/a>. It's my first &lt;em>real&lt;/em> rust project and I wanted an even more minimal feedreader.&lt;/p>
&lt;p>I haven't gotten around to figuring out users or authentication in the feedreader though. Despite this, it finally reached the point where I can use it as my main feedreader. However, I didn't exactly want an unauthenticated app hanging out on the internet for someone to ruin my day.&lt;/p>
&lt;p>If you don't want to make authentication, just make it inaccessible. This is where &lt;a href="https://tailscale.com/">Tailscale&lt;/a> comes in. I already run Tailscale on all the nodes, which is how I'm able to have a &lt;a href="https://www.joshkasuboski.com/posts/multi-region-k3s/">multi-region k3s cluster&lt;/a>. That doesn't make my pods routable though.&lt;/p>
&lt;p>Tailscale has an option for a &lt;a href="https://github.com/tailscale/tailscale/tree/main/docs/k8s#subnet-router">subnet router&lt;/a> that is actually highlighted as how to access all things k8s in the examples. This probably would have been nice (and I might still add it), but I wouldn't automatically get dns routing I believe.&lt;/p>
&lt;p>You can see how it's all put together in my &lt;a href="https://github.com/kasuboski/k8s-gitops/blob/main/default/feedreader/add-tailscale-proxy.yaml">k8s-gitops repo&lt;/a>. The gist is that you add a sidecar container that starts tailscale and in my case adds an &lt;code>iptables&lt;/code> rule which forwards all traffic to the app port. You can see my poor Doppler secret naming in there too where everything is using &lt;code>miniflux-secret&lt;/code>.&lt;/p>
&lt;h2 id="so-no-more-auth">So, no more auth?&lt;/h2>
&lt;p>I still want to add users to the feedreader so that you can have multiple users on one instance. It remains to be seen whether I'll just add basic auth or something more complicated. Tailscale let me focus on getting something usable quickly though.&lt;/p>
&lt;p>After seeing how convenient it was to expose a pod with a dns name, I also want to make a debugging tool injecting tailscale to pods. I could then finely be rid of the finicky &lt;code>kubectl port-forward&lt;/code>.&lt;/p></description></item><item><title>Multi-Region K3s</title><link>https://www.joshkasuboski.com/posts/multi-region-k3s/</link><pubDate>Thu, 11 Nov 2021 15:56:57 -0600</pubDate><guid>https://www.joshkasuboski.com/posts/multi-region-k3s/</guid><description>&lt;p>I had stood up a cluster in the cloud before moving. Now, I've added some nodes at home to round it out. Maybe I'm just in it for the buzzwords.&lt;/p>
&lt;h2 id="why-not-just-the-cloud">Why not just the cloud?&lt;/h2>
&lt;p>As previously mentioned in &lt;a href="https://www.joshkasuboski.com/posts/moving-cluster-to-the-cloud/">moving my cluster to the cloud&lt;/a>, I run a VM for the control plane and then a free Oracle ARM 4x24 VM as a worker. You might think this is more than enough (and you'd probably be right), but there are some things that make more sense to run at home.&lt;/p>
&lt;p>I currently run media services and backups at home. I also hope to mess around with more home automation things in the future. The media consumption is nice to be local, but it also saves on cloud network transfer costs. Overall, the cloud worker VM is going to be much more reliable than the small form factor PCs that make up my home region.&lt;/p>
&lt;h2 id="hows-it-actually-work">How's it actually work?&lt;/h2>
&lt;p>I install &lt;a href="https://tailscale.com/">Tailscale&lt;/a> on all of the machines. This lets them easily connect to each other as if they were on the same local network. The kubernetes API server only binds to the Tailscale interface so all of the workers are able to reach it. After setting this up, it mostly just worksâ„¢ï¸.&lt;/p>
&lt;p>The nodes are separated into a cloud and home region. Each region is broken down into zones for the cloud provider or location. That ends up cloud-oracle, cloud-racknerd, home-austin, home-wisconsin region-zone pairs. I'm then able to use those labels for scheduling decisions.&lt;/p>
&lt;p>So far I'm only using the zones for austin and wisconsin to pin my media options to the respective machines. Other things, including my RSS reader, are able to float between regions.&lt;/p>
&lt;h2 id="the-setup">The Setup&lt;/h2>
&lt;p>The cluster is still managed with GitOps from the repo &lt;a href="https://github.com/kasuboski/k8s-gitops">kasuboski/k8s-gitops&lt;/a>. I'm using ArgoCD to apply the manifests, but might mess with other options.&lt;/p>
&lt;p>I recently changed the secrets management from &lt;a href="https://github.com/bitnami-labs/sealed-secrets">SealedSecrets&lt;/a> to &lt;a href="https://www.doppler.com/">Doppler&lt;/a>. There wasn't anything wrong with SealedSecrets, but it felt less magic since I had to make sure to manage the keys and reencrypt on changes. I have a script under &lt;code>hack/&lt;/code> in the repo that manages importing the correct Doppler project token. After creating their &lt;code>DopplerSecret&lt;/code> CRD, the secrets then just show up.&lt;/p>
&lt;p>Previously, I had run a separate VM to act as the entrypoint to the cluster. Now I use a loadbalancer from Oracle that points to the free ARM VM there. The DNS then points to this loadbalancer. I still want to add a separate ingress locally so I can avoid always going out and back in, but haven't gotten around to it.&lt;/p>
&lt;p>I also still have 4 Raspberry Pis that I haven't setup yet since moving. The general layout is as below.&lt;/p>
&lt;figure>&lt;a href="homelab.png">
&lt;img src="homelab.png"
alt="Homelab Diagram"/> &lt;/a>
&lt;/figure>
&lt;h2 id="further-improvements">Further Improvements&lt;/h2>
&lt;p>I have a lot of the setup documented in &lt;a href="https://github.com/kasuboski/home-infra">kasuboski/home-infra&lt;/a>, but realized as I was setting up the machine in Austin that it leaves a lot to be desired. In particular, the setup for storage had me looking back through the command history of the Wisconsin machine to figure out what I had done.&lt;/p>
&lt;p>I'm thinking to make a tool that will set this up or at least walk me through it. The &lt;code>home-infra&lt;/code> repo needs to be cleaned up a little as well. It still contains instructions for multiple past iterations of my homelab.&lt;/p></description></item><item><title>Interact with Blockchain Apps in Javascript</title><link>https://www.joshkasuboski.com/posts/interact-blockchain-javascript/</link><pubDate>Mon, 30 Aug 2021 21:11:59 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/interact-blockchain-javascript/</guid><description>&lt;p>Interfacing with a blockchain can seem intimidating, but this dapp has a nice javascript sdk.&lt;/p>
&lt;h2 id="whats-it-do">What's it do?&lt;/h2>
&lt;p>I made a script that will check your collateral positions in &lt;a href="https://terra.mirror.finance/">Mirror Protocol&lt;/a>. Mirror is an app on the &lt;a href="https://www.terra.money/">Terra&lt;/a> blockchain that lets you trade stocks, but in reality they are tokens that try to stay pegged to the real life equivalent.&lt;/p>
&lt;p>There's an option to short an asset in Mirror. This requires that you put up collateral as you are essentially borrowing an asset and promise to pay it back later. The collateral you put up needs to maintain a certain ratio or it will be liquidated to pay your debt.&lt;/p>
&lt;p>If the asset you borrowed grows in value too fast you may need to add more collateral or close your position to not get liquidated. You can imagine you might want to know if you're getting close, before it happens, and while not checking constantly.&lt;/p>
&lt;h2 id="interacting-with-mirror">Interacting with Mirror&lt;/h2>
&lt;p>Mirror provides a Javascript SDK at &lt;a href="https://docs.mirror.finance/developer-tools/mirror.js">Mirror.js&lt;/a>. The documentation is a little lacking, but between reading the source code and the queries available on the actual &lt;a href="https://docs.mirror.finance/contracts/mint#positions">smart contract&lt;/a> you can find everything you need.&lt;/p>
&lt;p>Once you have this SDK, interacting with Mirror and Terra is pretty simple. The entire process of checking if your collateral ratio is safe is shown below.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-javascript" data-lang="javascript">&lt;span style="color:#a6e22e">async&lt;/span> &lt;span style="color:#66d9ef">function&lt;/span> &lt;span style="color:#a6e22e">checkPositions&lt;/span>(&lt;span style="color:#a6e22e">address&lt;/span>) {
&lt;span style="color:#75715e">// find positions
&lt;/span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// if short look up price asset vs collateral
&lt;/span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// check that collateral ratio is safe
&lt;/span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">positions&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">await&lt;/span> &lt;span style="color:#a6e22e">mirror&lt;/span>.&lt;span style="color:#a6e22e">mint&lt;/span>.&lt;span style="color:#a6e22e">getPositions&lt;/span>(&lt;span style="color:#a6e22e">address&lt;/span>);
&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">short_positions&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">positions&lt;/span>.&lt;span style="color:#a6e22e">positions&lt;/span>.&lt;span style="color:#a6e22e">filter&lt;/span>(&lt;span style="color:#a6e22e">pos&lt;/span> =&amp;gt; &lt;span style="color:#a6e22e">pos&lt;/span>.&lt;span style="color:#a6e22e">is_short&lt;/span>);
&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">info&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">await&lt;/span> Promise.&lt;span style="color:#a6e22e">all&lt;/span>(&lt;span style="color:#a6e22e">short_positions&lt;/span>.&lt;span style="color:#a6e22e">map&lt;/span>(&lt;span style="color:#a6e22e">async&lt;/span> &lt;span style="color:#a6e22e">pos&lt;/span> =&amp;gt; {
&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">collateralPrice&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">await&lt;/span> &lt;span style="color:#a6e22e">getCollateralPrice&lt;/span>(&lt;span style="color:#a6e22e">pos&lt;/span>.&lt;span style="color:#a6e22e">collateral&lt;/span>.&lt;span style="color:#a6e22e">info&lt;/span>.&lt;span style="color:#a6e22e">token&lt;/span>.&lt;span style="color:#a6e22e">contract_addr&lt;/span>);
&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">assetPrice&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">await&lt;/span> &lt;span style="color:#a6e22e">getAssetPrice&lt;/span>(&lt;span style="color:#a6e22e">pos&lt;/span>.&lt;span style="color:#a6e22e">asset&lt;/span>.&lt;span style="color:#a6e22e">info&lt;/span>.&lt;span style="color:#a6e22e">token&lt;/span>.&lt;span style="color:#a6e22e">contract_addr&lt;/span>);
&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">collateralAmount&lt;/span> &lt;span style="color:#f92672">=&lt;/span> Number.parseInt(&lt;span style="color:#a6e22e">pos&lt;/span>.&lt;span style="color:#a6e22e">collateral&lt;/span>.&lt;span style="color:#a6e22e">amount&lt;/span>);
&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">assetAmount&lt;/span> &lt;span style="color:#f92672">=&lt;/span> Number.parseInt(&lt;span style="color:#a6e22e">pos&lt;/span>.&lt;span style="color:#a6e22e">asset&lt;/span>.&lt;span style="color:#a6e22e">amount&lt;/span>);
&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">collateralValue&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">collateralAmount&lt;/span> &lt;span style="color:#f92672">*&lt;/span> &lt;span style="color:#a6e22e">collateralPrice&lt;/span>;
&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">assetValue&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">assetAmount&lt;/span> &lt;span style="color:#f92672">*&lt;/span> &lt;span style="color:#a6e22e">assetPrice&lt;/span>;
&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">collateralRatio&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">collateralValue&lt;/span> &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#a6e22e">assetValue&lt;/span>;
&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">minRatio&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">await&lt;/span> &lt;span style="color:#a6e22e">getMinCollateralRatio&lt;/span>(&lt;span style="color:#a6e22e">pos&lt;/span>.&lt;span style="color:#a6e22e">asset&lt;/span>.&lt;span style="color:#a6e22e">info&lt;/span>.&lt;span style="color:#a6e22e">token&lt;/span>.&lt;span style="color:#a6e22e">contract_addr&lt;/span>);
&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">safeRatio&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">minRatio&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">0.5&lt;/span>; &lt;span style="color:#75715e">// taken from mirror site
&lt;/span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">isSafe&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">collateralRatio&lt;/span> &lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#a6e22e">safeRatio&lt;/span>;
&lt;span style="color:#66d9ef">return&lt;/span> {
&lt;span style="color:#a6e22e">collateralPrice&lt;/span>,
&lt;span style="color:#a6e22e">assetPrice&lt;/span>,
&lt;span style="color:#a6e22e">collateralAmount&lt;/span>,
&lt;span style="color:#a6e22e">assetAmount&lt;/span>,
&lt;span style="color:#a6e22e">collateralValue&lt;/span>,
&lt;span style="color:#a6e22e">assetValue&lt;/span>,
&lt;span style="color:#a6e22e">collateralRatio&lt;/span>,
&lt;span style="color:#a6e22e">isSafe&lt;/span>,
};
}));
&lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">info&lt;/span>;
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>All of the information we need has mappings in Javascript so it feels like calling a function. If there's anything not exposed there is also a GraphQL API. If you want to try it out you can find it on GitHub at &lt;a href="https://github.com/kasuboski/mirror-watch">kasuboski/mirror-watch&lt;/a>. You just need to know a Terra address. It can be yours or someone else's since all data is public.&lt;/p>
&lt;h2 id="moving-forward">Moving Forward&lt;/h2>
&lt;p>I already made a similar script for &lt;a href="https://anchorprotocol.com/">Anchor Protocol&lt;/a>, which is also on Terra, at &lt;a href="https://github.com/kasuboski/anchor-watch">kasuboski/anchor-watch&lt;/a>. There are already projects building notifications and auto repayment strategies. There are also projects that are trying to combine Mirror Assets to make a pseudo ETF.&lt;/p>
&lt;p>I started messing with &lt;a href="https://academy.terra.money">Terra Academy&lt;/a>, where it walks you through building a smart contract in Rust. Maybe I'll end up doing something with that, but for now it's easy enough to interface just using the various APIs. No need to live on-chain.&lt;/p></description></item><item><title>Moving My Cluster to the Cloud</title><link>https://www.joshkasuboski.com/posts/moving-cluster-to-the-cloud/</link><pubDate>Thu, 05 Aug 2021 22:06:31 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/moving-cluster-to-the-cloud/</guid><description>&lt;p>I was going to move and didn't want to be without my rss reader (which I self host). To the cloud â˜ï¸ it is.&lt;/p>
&lt;h2 id="searching-for-free-vms">Searching for Free VMs&lt;/h2>
&lt;p>I had been thinking of using the generous Oracle free-tier to run a k3s cluster. Especially since they recently announced a free 4x24 ARM node. The entrypoint to my homelab has already been moved from Linode to a free Oracle VM. I wasn't able to get the free ARM node in my always-free account. I added a payment method and was able to provision it in a not always free region. The compute is free, but I pay for the disk which is like $2 a month.&lt;/p>
&lt;p>I had originally been thinking to use Postgresql as the storage for k3s, but it was using a ton of bandwidth between the Linode and Oracle. That's less of a concern now that I'm not using Linode, but I think I'll stick with sqlite. I'm less worried about their being an issue with it since it's hosted on a cloud VM.&lt;/p>
&lt;h2 id="setting-up-the-cluster">Setting up the Cluster&lt;/h2>
&lt;p>I ended up buying a VPS from RackNerd as part of a special for around $1.25 per month. It's 3x2.5 and comes with 6.5TB of bandwidth. I put the k3s server on there and am using the giant ARM node as only an agent.&lt;/p>
&lt;p>This time for my cluster, I set up Tailscale first. I then had the api server bind to the Tailscale interface so it's only accessible while connected to tailscale. This made my cilium networking not quite work and I ended up going back to their default tunnel option. Then it just worked.&lt;/p>
&lt;p>Since my cluster was already managed in git, I was able to just create a new branch and configure it for the new cluster. I took this time to move &lt;a href="https://github.com/kasuboski/k8s-gitops/tree/main">k8s-gitops&lt;/a> to using main as the default branch. Once I'm settled after the move I plan to add my Raspberry Pis and small form factor PCs to the cluster as well. One of the small form factor PCs will actually be moving to a different state as a backup.&lt;/p>
&lt;h2 id="finally-getting-the-rss-reader">Finally Getting the RSS Reader&lt;/h2>
&lt;p>After I had the cluster up and running, I was able to use my existing kubernetes yaml to deploy miniflux. I did need to reencrypt the secrets so that it would work with the key on the new cluster. I'm still thinking to use a separate secrets management solution, but haven't gotten around to it. Even though I now have cloud disks, I still didn't really want to manage the miniflux Postgresql database. My previous attempts to find a free postgres didn't pan out with &lt;a href="https://www.joshkasuboski.com/posts/migrating-to-free-heroku-postgres/">heroku&lt;/a>.&lt;/p>
&lt;p>I found that &lt;a href="https://supabase.io/pricing">Supabase&lt;/a> has a very nice free plan. The database you get also gives you full access. I ended up configuring the miniflux database the same way I did when running it myself. Transferring the data went smoothly. I scaled down miniflux in the existing cluster and then used standard postgres backup and import tools to go from the one I hosted to Supabase. After adding the new connection info to the miniflux secret, it just worked ðŸ§™.&lt;/p>
&lt;h2 id="moving-forward">Moving Forward&lt;/h2>
&lt;p>I still need to set everything up at my new place and join them to the cluster. There are a bunch of things that I don't want to run in the cloud, so I'm still deciding if I want to taint the cloud nodes and use tolerations or just rely on affinities. I did already label the cloud nodes with a topology region of cloud so I can use it for scheduling. Ultimately, I'm hoping I can have it so things that can run anywhere will be able to bounce between going whereever is most efficient.&lt;/p>
&lt;p>I'd also like to setup a separate ingress that is local only so I'm not always going out and back in to access cluster apps.&lt;/p></description></item><item><title>Migrating to Free Heroku Postgres (or not)</title><link>https://www.joshkasuboski.com/posts/migrating-to-free-heroku-postgres/</link><pubDate>Tue, 29 Jun 2021 15:17:39 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/migrating-to-free-heroku-postgres/</guid><description>&lt;p>I want to move my miniflux database to a managed service. Spoilers&amp;hellip; it didn't go well.&lt;/p>
&lt;h2 id="current-setup">Current setup&lt;/h2>
&lt;p>I currently have &lt;a href="https://miniflux.app/">miniflux&lt;/a> running in my &lt;a href="https://www.joshkasuboski.com/posts/home-k8s-raspberry-update/">raspberry pi kubernetes cluster&lt;/a>. Miniflux requires a Postgresql database. I haven't had great experiences running persistent storage on the Raspberry Pis so I opted to run the database on my thinkcentre instead. It's running as a container and hasn't been an issue since.&lt;/p>
&lt;p>There have been times however, where I have broken my entire homelab. The power grid in Texas has also been less than reliable ðŸ¥¶. It was rather annoying to be freezing and also not have access to my feed reader. Since I want my feed reader to be more available than my homelab permits, I'm naturally trying to extend to the cloud.&lt;/p>
&lt;h2 id="managing-the-database">Managing the Database&lt;/h2>
&lt;p>Free managed databases seem to be hard to come by. Many cloud providers offer them in the trial period, but not in the always-free plans. Heroku, however, has a pretty nice offering.&lt;/p>
&lt;p>You get 1GB of storage and 10,000 rows. You're even able to retain 2 backups. I checked my current miniflux database and it's only using 150 MB. There should be plenty of room to grow. I think miniflux also clears entries after a certain period.&lt;/p>
&lt;p>I created a Postgresql add-on in the Heroku console. They only really tell you how to connect from a Heroku app, which isn't what I'm after. I eventually found this help &lt;a href="https://devcenter.heroku.com/articles/connecting-to-heroku-postgres-databases-from-outside-of-heroku">doc&lt;/a> that pretty explicitly says you shouldn't rely on the connection info statically. The recommended option is to use the Heroku cli to get your connection string.&lt;/p>
&lt;p>At first, I tried to create a container that has the Heroku cli in it. Unfortunately, it wouldn't build for ARM, which I need for the Pis and my upcoming free Oracle ARM VM. I still built it for amd64 so if that's all you need it's at &lt;a href="https://github.com/kasuboski/heroku-cli">kasuboski/heroku-cli&lt;/a>.&lt;/p>
&lt;p>My next idea was to just call the API directly. There are a couple of clients to help. The Golang one hadn't been updated in years so I went with &lt;a href="https://github.com/heroku/node-heroku-client">nodejs&lt;/a>. You can see the project I made at &lt;a href="https://github.com/kasuboski/heroku-ps-url-grabber">kasuboski/heroku-ps-url-grabber&lt;/a>.&lt;/p>
&lt;p>It's a bit of a mouthful, but it will get the config variables associated with your Heroku app. In my case, it only has a &lt;code>DATABASE_URL&lt;/code> for the Postgresql addon. This little script means I can get the connection string and store it in a file that miniflux can read.&lt;/p>
&lt;h2 id="migrating-to-the-managed-database">Migrating to the Managed Database&lt;/h2>
&lt;p>After getting that working and building on ARM, I moved on to migrating the data. The Heroku cli has a nice &lt;a href="https://devcenter.heroku.com/articles/heroku-postgresql#pg-push">pg:push&lt;/a> command that will push from one Postgresql to the one in your account. It took me a second to figure out how to have it point to another host, but once it did everything went smoothly. An example command is below.&lt;/p>
&lt;pre>&lt;code>heroku pg:push postgres://user:pass@olddb/miniflux postgresql-random-id --app your-app
&lt;/code>&lt;/pre>&lt;p>I was adding an initContainer to my miniflux kubernetes deployment, when I noticed an email from Heroku. My database had 16,000 rows and would have &lt;code>INSERT&lt;/code> permission removed in 6 hours. The miniflux backup apparently contained 16,000 rows, far above the 10,000 free limit ðŸ˜’.&lt;/p>
&lt;p>The free Heroku Postgresql isn't going to work for me it seems.&lt;/p>
&lt;h2 id="next-steps">Next Steps&lt;/h2>
&lt;p>I still want to get my miniflux running elsewhere (or at least have the ability to easily switch it). I had been considering adding a cloud node to my cluster. Once that is setup, I will look into running miniflux on that including its postgres.&lt;/p>
&lt;p>I have been wanting to do something like &lt;a href="https://itnext.io/how-to-deploy-a-single-kubernetes-cluster-across-multiple-clouds-using-k3s-and-wireguard-a5ae176a6e81">this&lt;/a>. I already run &lt;a href="https://tailscale.com/">tailscale&lt;/a> on all of my nodes so running the kubernetes control plane in the cloud with nodes spread about may be the perfect solution.&lt;/p>
&lt;p>You can see what my tailscale setup is like in &lt;a href="https://www.joshkasuboski.com/posts/connect-home-cloud-tailscale/">Connect Your Home to the Cloud with Tailscale&lt;/a>. Oracle was currently out of capacity for ARM VMs, but hopefully I'll be able to provision soon.&lt;/p></description></item><item><title>M1 Macbook Air Setup</title><link>https://www.joshkasuboski.com/posts/m1-macbook-setup/</link><pubDate>Mon, 28 Jun 2021 15:10:16 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/m1-macbook-setup/</guid><description>&lt;p>I recently got an M1 Macbook Air. I wanted to try out a new development setup and track my dotfiles in git.&lt;/p>
&lt;h2 id="the-machine">The Machine&lt;/h2>
&lt;p>I got the standard config with the 7-core GPU. I went with the standard one largely because bumping to more RAM meant it wouldn't arrive for two weeks and I'm impatient. I don't plan to use this for anything too intensive so hopefully the 8GB will be enough.&lt;/p>
&lt;p>So far it's been great. I had kind of forgotten how small a 13.3&amp;rdquo; display would be, but the extra portability has been nice. The battery actually hasn't gone below 50% yet while I've been setting the machine up. I'm sure the only two usb-c ports will come to annoy me, but for now I'm just happy I didn't need a different charging standard.&lt;/p>
&lt;h2 id="the-config">The Config&lt;/h2>
&lt;p>I decided to use &lt;a href="https://www.chezmoi.io/">chezmoi&lt;/a> to manage the config for this machine. I had been meaning to do it for awhile on my WSL Ubuntu, but never got around to it. I'm very worried I won't be able to recreate the set up in WSL&amp;hellip; Using chezmoi, I can codify my dotfiles in a git repo that then gets applied. You can find the repo at &lt;a href="https://github.com/kasuboski/dotfiles">kasuboski/dotfiles&lt;/a>. I still need to work on it supporting more than just mac. I've read it can integrate nicely with codespaces so I'm especially interested in that or &lt;a href="https://github.com/containers/toolbox">toolbox&lt;/a>.&lt;/p>
&lt;p>In theory, once I have the config working for multiple types of machines, I should be able to do &lt;code>sh -c &amp;quot;$(curl -fsLS git.io/chezmoi)&amp;quot; -- init --apply kasuboski&lt;/code> and have a similar machine. It will be nice to have a similar config across the various devices I use.&lt;/p>
&lt;h2 id="the-tooling">The Tooling&lt;/h2>
&lt;p>You can see the packages I installed with brew in an &lt;a href="https://github.com/kasuboski/dotfiles/blob/main/run_once_before_10-install-packages-darwin.sh.tmpl">install script&lt;/a>. Chezmoi only runs the file when its contents change. Sending the list of packages to brew in this way makes it easy to only rerun when new packages are installed.&lt;/p>
&lt;p>The biggest change to my setup is probably using the &lt;a href="https://fishshell.com/">fish&lt;/a> shell. I had been using zsh in wsl, but like a lot of the built-in features of fish. I've really enjoyed seeing the completions while learning chezmoi for instance. I'm a little worried I'll run into issues using something that's less popular. I've already found the kubectl aliases I usually use don't have an easy fish equivalent, but there is at least &lt;a href="https://github.com/evanlucas/fish-kubectl-completions">kubectl completions&lt;/a> available. It's even available to install as a &lt;a href="https://github.com/jorgebucaran/fisher">fisher&lt;/a> plugin.&lt;/p>
&lt;p>I also installed a bunch of the rust cli tools that replace common unix commands. My most frequently used is certainly &lt;a href="https://github.com/Peltoche/lsd">lsd&lt;/a> that makes your &lt;code>ls&lt;/code> output much prettier. I have &lt;code>ls&lt;/code> aliased to &lt;code>lsd&lt;/code> so I use it exclusively now.&lt;/p>
&lt;p>I'm also taking this opportunity to try and get back into using vim or rather &lt;a href="https://neovim.io/">neovim&lt;/a>. Usually, I use VSCode and I'll probably end up installing it again, but want to see if I can stay mainly in the terminal. I still haven't really figured out all of my neovim plugins. I need to set up &lt;a href="https://github.com/neoclide/coc.nvim">coc&lt;/a> for neovim to have VSCode like language server protocol and extension features.&lt;/p>
&lt;h2 id="m1-thoughts">M1 Thoughts&lt;/h2>
&lt;p>This mac is incredibly responsive. I haven't done anything stressing yet, but all apps have opened instantly. I did have to use Rosetta for Discord and it's probably the only thing I've noticed is slower than I expect. I'm glad I waited a bit so things had some time to be compatible with darwin arm64.&lt;/p>
&lt;p>I already mentioned the battery life, but coming from my Mid-2012 Macbook Pro it's night and day. That macbook was starting to show its age in general use as well.&lt;/p>
&lt;p>Using Touch ID to unlock the mac is also something I hadn't really thought about. It's very nice to come back and just touch the keyboard for it to unlock. Maybe it will even get me to use Apple Pay.&lt;/p></description></item><item><title>Rackmounting the Raspberry PIs</title><link>https://www.joshkasuboski.com/posts/rackmount-pi/</link><pubDate>Wed, 13 Jan 2021 14:04:08 -0600</pubDate><guid>https://www.joshkasuboski.com/posts/rackmount-pi/</guid><description>&lt;p>My stack of Raspberry PIs was getting a little tippy&amp;hellip; A bracket from Germany and some 3D printing later and it's good to go.&lt;/p>
&lt;p>&lt;em>There are affiliate links below that I may earn a commission from.&lt;/em>&lt;/p>
&lt;h2 id="previously">Previously&lt;/h2>
&lt;p>My Raspberry PIs had been just sitting on a shelf in the server rack. They were mounted together, but the stack of them would often slide around on the shelf. The stack can be seen below sitting on top of the switch. Getting the ethernet cables to cooperate in this orientation was not pretty.&lt;/p>
&lt;p>&lt;a href="raspberry-pi-stack.jpg">&lt;img src="raspberry-pi-stack.jpg" alt="raspberry pi stack">&lt;/a>&lt;/p>
&lt;p>As mentioned in &lt;a href="https://www.joshkasuboski.com/posts/server-rack-2/">server rack update&lt;/a>, I bought a &lt;a href="https://www.musicstore.de/de_DE/EUR/DAP-2-HE-Rackblende-f-Modulsystem-10-Segmente-MP-1/art-PAH0017160-000;pgid=WBtg67.syLdSRpoV6L_EAtys0000YDPT2oVh">bracket&lt;/a> from a random german store. Well it finally arrived, but then I had to actually get the mounts to put in.&lt;/p>
&lt;h2 id="mounting-the-pis">Mounting the PIs&lt;/h2>
&lt;p>I had the &lt;code>.stl&lt;/code> from &lt;a href="https://www.kaibader.de/3d-printed-raspberry-pi-rack-mount-with-heat-sink-passive-cooling/">here&lt;/a> printed using &lt;a href="https://www.makexyz.com/">makexyz.com&lt;/a>. It was pretty convenient and arrived in like a week. The actual quality of the print wasn't the greatest with some significant artifacts on the face.&lt;/p>
&lt;p>After lining it up to the rack, I realized it was also too tall to align with the screw holes. On top of that, it seemed the holes in the bracket weren't actually tapped, so the included screws didn't fit.&lt;/p>
&lt;p>I ended up buying a &lt;a href="https://www.amazon.com/gp/product/B07WN8BGWJ?tag=kasuboski-20&amp;amp;geniuslink=true">taps set&lt;/a>. That tap was able to thread the holes for M4. It was easy enough, but took some time.&lt;/p>
&lt;p>&lt;a href="tap-holes.jpg">&lt;img src="tap-holes.jpg" alt="tapping holes">&lt;/a>&lt;/p>
&lt;p>The next issue was the actual mount was too big. Only one screw was able to be attached. I ended up buying a 3D printer instead of using a service. I got the &lt;a href="https://www.amazon.com/gp/product/B08GLH4WFG?tag=kasuboski-20&amp;amp;geniuslink=true">Creality CR6-SE&lt;/a>. Now able to print my own parts, I bought a &lt;a href="https://www.amazon.com/gp/product/B07DFFYCXS?tag=kasuboski-20&amp;amp;geniuslink=true">digital caliper&lt;/a> and modified the mount in &lt;a href="https://www.freecadweb.org/">FreeCAD&lt;/a> to match the dimensions. After 2 iterations, I ended up with a good fit.&lt;/p>
&lt;p>&lt;a href="mount-in-bracket.jpg">&lt;img src="mount-in-bracket.jpg" alt="raspberry pi mount">&lt;/a>&lt;/p>
&lt;p>I attached the Raspberry PIs to the mount using the top part of this &lt;a href="https://www.amazon.com/gp/product/B085XPHY77?tag=kasuboski-20&amp;amp;geniuslink=true">heatsink case&lt;/a>. The screws that came with easily fit through the 3D printed part. I did notice a few degree difference with and without the heatsink so it's not just to look good ðŸ˜‰.&lt;/p>
&lt;p>&lt;a href="pi-in-case.jpg">&lt;img src="pi-in-case.jpg" alt="raspberry pi case">&lt;/a>&lt;/p>
&lt;p>With all of that done, I now have all of the Raspberry PIs mounted. I bought a &lt;a href="https://www.amazon.com/gp/product/B00LT6432U?tag=kasuboski-20&amp;amp;geniuslink=true">full length shelf&lt;/a> that holds the switch for the pis and a thinkcentre pc. My entire rack is able to be pulled out from the wall now without worrying about something toppling.&lt;/p>
&lt;p>&lt;a href="rack-update.jpg">&lt;img src="rack-update.jpg" alt="server rack">&lt;/a>&lt;/p></description></item><item><title>Re-provisioning the Cluster</title><link>https://www.joshkasuboski.com/posts/cluster-reprovisioning/</link><pubDate>Tue, 15 Dec 2020 12:29:47 -0600</pubDate><guid>https://www.joshkasuboski.com/posts/cluster-reprovisioning/</guid><description>&lt;p>The SD card died in the server node of my cluster ðŸ˜¢. Time to start again.&lt;/p>
&lt;h2 id="tragedy">Tragedy&lt;/h2>
&lt;p>The SD card in blueberry died taking down the kubernetes api with it. The workloads were still running so it didn't matter too much, but I couldn't do anything new.&lt;/p>
&lt;p>I didn't have a backup for the SD card&amp;hellip; so was going to have to start again. I took the opportunity to try out some new things. I had been wanting to try out Ubuntu on the Raspberry Pi since they recently announced official support.&lt;/p>
&lt;p>My thought with Ubuntu is to standardize more on the various machines I run. Previously, Raspbian had to have its own process for all automations.&lt;/p>
&lt;h2 id="re-provisioning">Re-provisioning&lt;/h2>
&lt;p>After deciding to go with Ubuntu, I needed to flash the sd cards with ubuntu and reinstall k3s. I used a pre-installed image of Ubuntu so was able to have them boot headlessly using cloud-init.&lt;/p>
&lt;p>After flashing, I could edit the cloud-init files so the pi would come up with the correct hostname and my ssh keys. I used this &lt;a href="https://github.com/billimek/homelab-infrastructure/tree/master/k3s/arm64">repo&lt;/a> from &lt;a href="https://github.com/billimek">billimek&lt;/a>.&lt;/p>
&lt;h2 id="installing-k3s">Installing k3s&lt;/h2>
&lt;p>I decided to install &lt;a href="https://docs.cilium.io/en/v1.9/gettingstarted/k3s/">cilium&lt;/a> as the cni instead of the bundled flannel. I initially wanted to use the multi-cluster features of cilium, but that seems to require running etcd separately&amp;hellip; so I will have to figure out if that's worth running on the Raspberry Pis. I was able to use the Hubble UI to have real-time telemetry info. In the future, I want to use the Cilium cluster-wide network policies.&lt;/p>
&lt;p>Cilium on k3s requires disabling the built-in flannel. I had to use the deprecated &lt;code>--no-flannel&lt;/code> since &lt;code>--flannel-backend=none&lt;/code> didn't seem to be supported. I also had to manually install the loopback cni plugin from &lt;a href="https://github.com/containernetworking/plugins">containernetworking/plugins&lt;/a>.&lt;/p>
&lt;h2 id="next-steps-">Next Steps ðŸ¦¶&lt;/h2>
&lt;p>I started making a new &lt;code>home-infra&lt;/code> repo where I'll start putting the steps (hopefully automated). Currently, just doing everything manually wasn't all that bad. Getting the stateless workloads running again was basically reinstalling ArgoCD and then pointing it at my gitops &lt;a href="https://github.com/kasuboski/k8s-gitops">repo&lt;/a>.&lt;/p>
&lt;p>One thing that was a little troublesome is remembering to label the node corresponding to the fast and slow storage. It was nice that my workloads requiring storage did land on the same node again, but it would be better for this to happen automatically. OpenEBS could probably help with this using the automatic detection of block devices and then letting you map that to storage classes.&lt;/p>
&lt;p>Manually setting up the folders for the &lt;a href="https://github.com/rancher/local-path-provisioner">local-path-provisioner&lt;/a> is pretty convenient though. I could probably just use &lt;a href="https://kubernetes.io/blog/2019/04/04/kubernetes-1.14-local-persistent-volumes-ga/">local persistent volumes as well&lt;/a> ðŸ¤·â€â™‚ï¸.&lt;/p></description></item><item><title>Removing OpenEBS</title><link>https://www.joshkasuboski.com/posts/removing-openebs/</link><pubDate>Tue, 01 Dec 2020 16:37:25 -0600</pubDate><guid>https://www.joshkasuboski.com/posts/removing-openebs/</guid><description>&lt;p>My OpenEBS install seemingly stopped working. ðŸ”¥&lt;/p>
&lt;h2 id="discovery">Discovery&lt;/h2>
&lt;p>I actually didn't really notice it had stopped working until two days later. I have monitoring set up for some sites, but it was only testing that the page loaded, not that data loaded. Lesson learned there I guess.&lt;/p>
&lt;p>The pods showed as having the storage attached, but it didn't work. Looking for the status of the various OpenEBS objects revealed that it was not healthy. I ran a couple commands that were mentioned in the kubernetes slack, but couldn't get it back.&lt;/p>
&lt;p>Luckily, I had recently backed up the data. You can see how at &lt;a href="https://www.joshkasuboski.com/posts/kubectl-cp/">copy files using kubectl&lt;/a>.&lt;/p>
&lt;h2 id="moving-on">Moving On&lt;/h2>
&lt;p>I decided I wanted to simplify the storage while I was redoing it anyway. In manually mounting my usb drives, I discovered that one was now permanently readonly. This was almost certainly the problem with OpenEBS, but I'd already uninstalled everything ðŸ¤·â€â™‚ï¸.&lt;/p>
&lt;p>I'm now using the &lt;a href="https://github.com/rancher/local-path-provisioner">local-storage-path&lt;/a> provisioner that's included with k3s. One Pi has the same model of microcenter usb drive as the one that failed and the other a &lt;a href="https://kit.co/kasuboski/homelab/corsair-flash-voyage">Corsair Drive&lt;/a> that I had seen benchmarked as fast specifically on a Raspberry Pi.&lt;/p>
&lt;p>Apps with storage are specifically pinned to either the slow disk node or the fast disk node. The apps on the fast disk node had issues with the OpenEBS and slow usb drive setup. They now perform significantly better.&lt;/p>
&lt;p>Just a note, that uninstalling OpenEBS took me quite some time&amp;hellip; I think maybe I did it in the wrong order because basically all resources got stuck with a finalizer that I then had to manually remove.&lt;/p>
&lt;p>I'd like to try OpenEBS again, but maybe in a setup where I actually have three storage nodes and faster disks.&lt;/p>
&lt;p>I'm excited to try Prometheus again, since previously the storage was just too slow. Although, I have plans to run it with Thanos so that most of the data will be in Minio (backed by my nas).&lt;/p></description></item><item><title>Add Multi-Arch Dependencies Easily</title><link>https://www.joshkasuboski.com/posts/easy-multi-arch-deps/</link><pubDate>Thu, 19 Nov 2020 11:02:45 -0600</pubDate><guid>https://www.joshkasuboski.com/posts/easy-multi-arch-deps/</guid><description>&lt;p>I wanted to build a multi-arch docker image for media transcoding. Time to get the dependencies from someone who already did it.&lt;/p>
&lt;h2 id="the-goal">The Goal&lt;/h2>
&lt;p>I wanted a multi-arch image to run &lt;a href="https://github.com/mdhiggins/sickbeard_mp4_automator">mdhiggins/sickbeard_mp4_automator&lt;/a>. I just needed the &lt;code>manual.py&lt;/code> script not the radarr integration. The images published by mdhiggins are based on images like the linuxserver/radarr image and aren't multi-arch.&lt;/p>
&lt;p>You can skip ahead to just see the Dockerfile at &lt;a href="https://github.com/kasuboski/manual-sma/blob/main/Dockerfile">kasuboski/manual-sma&lt;/a>.&lt;/p>
&lt;p>The main issue for making the image multi-arch is &lt;code>ffmpeg&lt;/code>. In the &lt;a href="https://github.com/mdhiggins/radarr-sma">mdhiggins/radarr-sma&lt;/a> Dockerfile, it is always downloading the amd64 version of ffmpeg. This obviously won't go well for other architectures.&lt;/p>
&lt;p>There are ffmpeg builds published for other architectures. You would just need to make sure to download the correct one.&lt;/p>
&lt;h2 id="the-lazy-solution">The Lazy Solution&lt;/h2>
&lt;p>It wouldn't be too bad to figure out which architecture is being built and then download the correct version of ffmpeg. It is one more script to maintain though.&lt;/p>
&lt;p>I noticed the linuxserver repo has a multi-arch ffmpeg container already. They include a statically compiled ffmpeg so getting it into my image is as easy as copying the binary.&lt;/p>
&lt;p>Dockerfiles have a &lt;code>COPY --from=&amp;lt;image&amp;gt;&lt;/code> option. This lets you copy files from another image. That image will automatically be the correct one for your architecture (as long as the image supports it).&lt;/p>
&lt;p>So instead of figuring out the correct architecture and downloading the corresponding ffmpeg. You can just add &lt;code>FROM linuxserver/ffmpeg as ffmpeg&lt;/code> and then &lt;code>COPY --from=ffmpeg /usr/local/bin/ff* /usr/local/bin/&lt;/code>.&lt;/p>
&lt;p>This will copy both ffmpeg and ffprobe to the built container.&lt;/p>
&lt;p>To build the image, I used the same method as my &lt;a href="https://www.joshkasuboski.com/posts/build-multiarch-image/">building multiarch images post&lt;/a>. Basically, setting up docker buildx in GitHub Actions. You can see the workflow at &lt;a href="https://github.com/kasuboski/manual-sma/blob/main/.github/workflows/docker.yml">kasuboski/manual-sma&lt;/a>.&lt;/p></description></item><item><title>Leaving Feedly for Miniflux</title><link>https://www.joshkasuboski.com/posts/switching-to-miniflux/</link><pubDate>Fri, 13 Nov 2020 13:57:42 -0600</pubDate><guid>https://www.joshkasuboski.com/posts/switching-to-miniflux/</guid><description>&lt;p>I've wanted to move away from Feedly for awhile and finally found my alternative in Miniflux.&lt;/p>
&lt;h2 id="why-the-move">Why the move&lt;/h2>
&lt;p>I had been having issues with the Feedly app where it would suddenly sign me out and take a while to log back in. Apparently that's all it takes for me to drop a service&amp;hellip; I also wanted to run something myself and possibly build on top of it.&lt;/p>
&lt;p>I had contemplated moving to microsub as outlined in my &lt;a href="https://www.joshkasuboski.com/posts/replacing-feedly/">replacing Feedly&lt;/a> post. I tried out &lt;a href="https://github.com/pstuifzand/ekster">ekster&lt;/a> and I think it's still running ðŸ¤·â€â™‚ï¸. I didn't like the readers I tried and wasn't a fan of how ekster doesn't seem to store data persistently.&lt;/p>
&lt;h2 id="moving-to-miniflux">Moving to Miniflux&lt;/h2>
&lt;p>I recently found &lt;a href="https://miniflux.app/">miniflux&lt;/a>. It bills itself as &amp;ldquo;a minimalist and opinionated feed reader&amp;rdquo;. Minimalist and opinionated is all I can hope to be. Thankfully the &lt;a href="https://miniflux.app/opinionated.html">opinions&lt;/a> aligned pretty well with mine.&lt;/p>
&lt;p>It's just a go binary and PostgreSQL. I run the binary on my &lt;a href="https://www.joshkasuboski.com/posts/home-k8s-raspberry-update/">kubernetes cluster&lt;/a> and PostgreSQL on a Lenovo Thinkcentre I recently bought. I'm still not super happy in my openEBS storage so am running the database using &lt;a href="https://podman.io/">podman&lt;/a> with a volume mount directly from the node.&lt;/p>
&lt;p>Running containers as a non-root account is pretty easy with podman and can be managed by systemd. I did the below to set it up.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">podman create --name postgres -v /home/postgres/postgres:/bitnami/postgresql -e POSTGRESQL_PASSWORD&lt;span style="color:#f92672">=&lt;/span>&amp;lt;root password&amp;gt; -p 5432:5432 bitnami/postgresql:13
podman generate systemd postgres -n
&lt;/code>&lt;/pre>&lt;/div>&lt;p>You'll notice I'm using the bitnami postgresql image. This image doesn't run as root ðŸ‘. The output of the &lt;code>podman generate systemd&lt;/code> can then be copied to &lt;code>$HOME/.config/systemd/user&lt;/code>. More info can be found on the podman &lt;a href="http://docs.podman.io/en/latest/markdown/podman-generate-systemd.1.html#installation-of-generated-systemd-unit-files">site&lt;/a>.&lt;/p>
&lt;p>I like the minimal UI and it works pretty well on mobile as well. The delay in fetching feeds took a little getting used to. I did lower the thresholds, which was just an environment variable config. You can see my kubernetes config in &lt;a href="https://github.com/kasuboski/k8s-gitops/blob/master/default/miniflux/deploy.yaml">kasuboski/k8s-gitops&lt;/a>&lt;/p>
&lt;p>&lt;img src="miniflux.png" alt="miniflux">&lt;/p>
&lt;h2 id="moving-forward-">Moving Forward ðŸ±â€ðŸ&lt;/h2>
&lt;p>With Feedly I had a limited set of feed categories because that was a limitation of the free plan. I want to decide what I actually want now that I have no limits with miniflux.&lt;/p>
&lt;p>There is also an API that I want to build some things around. My first thought would be to make a way for others to follow what I follow as well.&lt;/p></description></item><item><title>Copy files to and from Kubernetes Pods with kubectl</title><link>https://www.joshkasuboski.com/posts/kubectl-cp/</link><pubDate>Thu, 05 Nov 2020 20:46:06 -0600</pubDate><guid>https://www.joshkasuboski.com/posts/kubectl-cp/</guid><description>&lt;p>I wanted a simple backup of some OpenEBS volumes. Why not just copy them out.&lt;/p>
&lt;h2 id="why-bother">Why bother&lt;/h2>
&lt;p>I run a number of things on my &lt;a href="https://www.joshkasuboski.com/posts/home-k8s-raspberry-pi/">Raspberry Pi kubernetes cluster&lt;/a> that require storage. The main one I actually care about is my &lt;a href="https://github.com/usefathom/fathom">fathom&lt;/a> instance.&lt;/p>
&lt;p>It has the website analytics for my site &lt;a href="https://www.joshkasuboski.com">joshkasuboski.com&lt;/a>. It's certainly not the end of the world to lose this information, but I'd really rather not. Previously, I had tried to get &lt;a href="https://velero.io/">velero&lt;/a> setup.&lt;/p>
&lt;p>I tried to use both the OpenEBS specific plugin and the generic restic. The OpenEBS one failed because my setup couldn't take snapshots. My USB drives are pretty slow so I just chalk it up to that. The restic option seemed to work, but it didn't actually restore data in my test. In addition, it kept exceeding my &lt;a href="https://www.backblaze.com/">backblaze&lt;/a> s3 api limits.&lt;/p>
&lt;p>Anyway, I just wanted something simple before I upgraded OpenEBS. Then I found the &lt;code>kubectl cp&lt;/code> command.&lt;/p>
&lt;h2 id="backing-up-the-volumes">Backing up the volumes&lt;/h2>
&lt;p>I manually backed up all of my persistent volumes. This is only five items for me, but could get out of hand quickly. It was really as simple as running the command for each and storing the files locally.&lt;/p>
&lt;p>Copying files from a pod to your machine
&lt;code>kubectl cp &amp;lt;namespace&amp;gt;/&amp;lt;podname&amp;gt;:/mount/path /local/path&lt;/code>&lt;/p>
&lt;p>I did this for all paths I cared about. If something goes wrong you can always copy the files back to the pod. It's the same command just swapping source and destination.&lt;/p>
&lt;p>Copying files to a pod from your machine
&lt;code>kubectl cp /local/path &amp;lt;namespace&amp;gt;/&amp;lt;podname&amp;gt;:/mount/path&lt;/code>&lt;/p>
&lt;p>I still hope to get OpenEBS snapshots working in the future, but this worked great for now.&lt;/p></description></item><item><title>Client Side Shopping Cart</title><link>https://www.joshkasuboski.com/posts/client-side-cart-1/</link><pubDate>Thu, 29 Oct 2020 14:15:57 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/client-side-cart-1/</guid><description>&lt;p>I have my beer can working as a buy button, but what if you want to add products to a cart first.&lt;/p>
&lt;h2 id="but-why">But Why&lt;/h2>
&lt;p>If you're trying to sell things easily (and cheaply) you can connect Stripe directly to a product page. You can see an example of this from the &lt;a href="https://www.joshkasuboski.com/posts/stripe-beer-money/">stripe beer money&lt;/a> article.&lt;/p>
&lt;p>The main downside is that a customer would have to buy one thing at a time. We can add a cart, but still don't want to require a server.&lt;/p>
&lt;h2 id="creating-a-cart">Creating a Cart&lt;/h2>
&lt;p>For a cart, we just need to keep track of items added and how many of each. We can store this information in &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage">localstorage&lt;/a>. This means if a user comes back to the page, their cart will still be there.&lt;/p>
&lt;p>I tested this out with a sample store page. You can see the code at &lt;a href="https://github.com/kasuboski/client-side-cart-example">kasuboski/client-side-cart-example&lt;/a>.&lt;/p>
&lt;p>&lt;a href="example-store.png">&lt;img src="example-store.png" alt="example store">&lt;/a>&lt;/p>
&lt;p>It's basically a single &lt;code>index.html&lt;/code> with &lt;code>javascript&lt;/code>. It looks for products by finding &lt;code>buttons&lt;/code> with &lt;code>data&lt;/code> attributes. These attributes specify the product id, description and price.&lt;/p>
&lt;p>When a user clicks the button, the item is added to the cart. This just loads the cart from &lt;code>localstorage&lt;/code> updates the item quantity and saves it back.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-js" data-lang="js">&lt;span style="color:#66d9ef">function&lt;/span> &lt;span style="color:#a6e22e">addToCart&lt;/span>(&lt;span style="color:#a6e22e">data&lt;/span>) {
&lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">cart&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">getCart&lt;/span>()
&lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">prevQuantity&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">cart&lt;/span>[&lt;span style="color:#a6e22e">data&lt;/span>.&lt;span style="color:#a6e22e">id&lt;/span>] &lt;span style="color:#f92672">?&lt;/span> &lt;span style="color:#a6e22e">cart&lt;/span>[&lt;span style="color:#a6e22e">data&lt;/span>.&lt;span style="color:#a6e22e">id&lt;/span>].&lt;span style="color:#a6e22e">quantity&lt;/span> &lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>;
&lt;span style="color:#a6e22e">cart&lt;/span>[&lt;span style="color:#a6e22e">data&lt;/span>.&lt;span style="color:#a6e22e">id&lt;/span>] &lt;span style="color:#f92672">=&lt;/span> {
&lt;span style="color:#a6e22e">quantity&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">prevQuantity&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>,
&lt;span style="color:#a6e22e">data&lt;/span>,
}
&lt;span style="color:#a6e22e">localStorage&lt;/span>.&lt;span style="color:#a6e22e">setItem&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;cart&amp;#39;&lt;/span>, &lt;span style="color:#a6e22e">JSON&lt;/span>.&lt;span style="color:#a6e22e">stringify&lt;/span>(&lt;span style="color:#a6e22e">cart&lt;/span>));
&lt;span style="color:#a6e22e">populateCart&lt;/span>();
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>populateCart&lt;/code> function sets up the cart area every time. There isn't anything fancy here&amp;hellip; it just deletes all of the cart elements and recreates based on what's in &lt;code>localstorage&lt;/code>.&lt;/p>
&lt;h2 id="next-steps-">Next Steps ðŸ¦¶&lt;/h2>
&lt;p>This works as a generic cart&amp;hellip; but you can't buy anything. I'm going to make an example store to show buying items using Stripe.&lt;/p>
&lt;p>Each item will need a Stripe Price and then when you Checkout it will call the Stripe redirect. Eventually, I want to make it easier to integrate as well. Maybe making this an actual library.&lt;/p></description></item><item><title>Scan Images in my GitOps Repo</title><link>https://www.joshkasuboski.com/posts/trivy-gitops-repo-scan/</link><pubDate>Thu, 22 Oct 2020 14:41:31 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/trivy-gitops-repo-scan/</guid><description>&lt;p>Scanning the container images deployed to my cluster used to be manual. Now it happens automatically every night. ðŸ±â€ðŸ&lt;/p>
&lt;h2 id="how-to-scan">How to Scan&lt;/h2>
&lt;p>I use Trivy to scan container images. I wrote about scanning my GitOps repo for images earlier &lt;a href="https://www.joshkasuboski.com/posts/scan-images-in-files/">here&lt;/a>. Basically, there's a fancy grep that matches &lt;code>image: (&amp;lt;name&amp;gt;)&lt;/code> and that name is sent to Trivy.&lt;/p>
&lt;p>My GitOps repo is on GitHub at &lt;a href="https://github.com/kasuboski/k8s-gitops">kasuboski/k8s-gitops&lt;/a>. It seemed natural to run the scan periodically using GitHub Actions. The scan will happen on every push and every night.&lt;/p>
&lt;p>I needed a way to exclude an image that wasn't able to scan on an x86 host. The one liner from my previous post needed a &lt;code>grep -v&lt;/code> to exclude certain patterns.&lt;/p>
&lt;h2 id="making-the-github-actions-workflow">Making the GitHub Actions Workflow&lt;/h2>
&lt;p>I had a lot of trouble getting &lt;code>ack&lt;/code> configured in a runner. I ended up making a docker image that downloads trivy, finds the images, and scans them.&lt;/p>
&lt;p>This image has its own repo &lt;a href="https://github.com/kasuboski/trivy-scan-dir">kasuboski/trivy-scan-dir&lt;/a>. If you just want to scan a repo you can run &lt;code>docker run -it --rm -v /path/to/yaml:/gitops -e EXCLUDED='no/scan also/noscan' kasuboski/trivy-scan-dir&lt;/code>.&lt;/p>
&lt;p>To run this in a workflow, add the below step.&lt;/p>
&lt;pre>&lt;code>- name: Scan Images
uses: docker://kasuboski/trivy-scan-dir:latest
env:
EXCLUDED: 'no/scan also/noscan'
&lt;/code>&lt;/pre>&lt;p>My full workflow can be found in &lt;a href="https://github.com/kasuboski/k8s-gitops/blob/master/.github/workflows/scan-images.yaml">kasuboski/k8s-gitops&lt;/a>. It triggers on &lt;code>workflow_dispatch&lt;/code>, &lt;code>cron&lt;/code>, and &lt;code>push&lt;/code> to yaml files.&lt;/p>
&lt;p>Workflow Dispatch lets you run the workflow manually from the GitHub Actions UI. This was really convenient for testing. The cron schedule runs every morning at 4:03am.&lt;/p>
&lt;p>&lt;a href="manual-workflow-run.png">&lt;img src="manual-workflow-run.png" alt="manual trigger">&lt;/a>&lt;/p>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>This workflow has alerted me to multiple vulnerabilities. If the workflow fails, I get an email and then can look into updating the image.&lt;/p>
&lt;p>The results even look pretty decent in the GitHub app so I can tell which images I need to be worried about. An example failing run is shown below.&lt;/p>
&lt;p>&lt;a href="failed-image-scan.png">&lt;img src="failed-image-scan.png" alt="failed run">&lt;/a>&lt;/p>
&lt;p>I still want to add something in cluster to enforce only the images I want are running. Finding the images to scan also needs to be more robust. For instance, some images only show up once manifests are rendered.&lt;/p></description></item><item><title>Accept Beer Money with Stripe - Sans Server</title><link>https://www.joshkasuboski.com/posts/stripe-beer-money/</link><pubDate>Thu, 15 Oct 2020 10:51:52 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/stripe-beer-money/</guid><description>&lt;p>I wanted to explore Stripe Payments, but didn't want to mess with a server. You can see the result as the little beer can in the footer ðŸ˜‰.&lt;/p>
&lt;h2 id="how-it-works">How It Works&lt;/h2>
&lt;p>Stripe has a Checkout option where you redirect to their page and it has your product and a payment form. This means you don't have to deal with &lt;a href="https://www.pcisecuritystandards.org/">PCI&lt;/a> anything and basically just have to redirect correctly. They have a library to handle the redirect as well, so it's fairly easy.&lt;/p>
&lt;p>You need to place a button that when clicked calls the Stripe Javascript library. Since I'm &amp;ldquo;selling&amp;rdquo; beer money, I put a little beer can in my site footer.&lt;/p>
&lt;p>&lt;img src="footer-beercan.png" alt="footer-beercan">&lt;/p>
&lt;h2 id="adding-to-your-site">Adding to Your Site&lt;/h2>
&lt;p>I followed this &lt;a href="https://stripe.com/docs/payments/checkout/client">guide&lt;/a> from Stripe. It was a little difficult to find navigating the Stripe Docs, but searching Stripe Checkout without server brought me there.&lt;/p>
&lt;p>I won't reiterate the guide, but basically you use the Stripe Dashboard to make a Product that has a Price. That price will then have an ID that you need. The dashboard will also generate the code snippet with the price ID and your API ID filled in. My edited snippet is below.&lt;/p>
&lt;p>You'll notice it also expects a success and cancel URL. I added two pages that just say success, and uh oh respectively.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-html" data-lang="html">&amp;lt;&lt;span style="color:#f92672">script&lt;/span> &lt;span style="color:#a6e22e">src&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;https://js.stripe.com/v3&amp;#34;&lt;/span>&amp;gt;&amp;lt;/&lt;span style="color:#f92672">script&lt;/span>&amp;gt;
&amp;lt;&lt;span style="color:#f92672">script&lt;/span>&amp;gt;
(&lt;span style="color:#66d9ef">function&lt;/span> () {
&lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">DOMAIN&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;https://www.joshkasuboski.com/&amp;#39;&lt;/span>;
&lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">key&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;&amp;lt;pk_livekey&amp;gt;&amp;#39;&lt;/span>;
&lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">price&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;&amp;lt;price_key&amp;gt;&amp;#39;&lt;/span>;
&lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">stripe&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">Stripe&lt;/span>(&lt;span style="color:#a6e22e">key&lt;/span>);
&lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">checkoutButton&lt;/span> &lt;span style="color:#f92672">=&lt;/span> document.&lt;span style="color:#a6e22e">getElementById&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;checkout-button-beermoney&amp;#39;&lt;/span>);
&lt;span style="color:#a6e22e">checkoutButton&lt;/span>.&lt;span style="color:#a6e22e">addEventListener&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;click&amp;#39;&lt;/span>, &lt;span style="color:#66d9ef">function&lt;/span> () {
&lt;span style="color:#75715e">// When the customer clicks on the button, redirect
&lt;/span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// them to Checkout.
&lt;/span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">stripe&lt;/span>.&lt;span style="color:#a6e22e">redirectToCheckout&lt;/span>({
&lt;span style="color:#a6e22e">lineItems&lt;/span>&lt;span style="color:#f92672">:&lt;/span> [{ &lt;span style="color:#a6e22e">price&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">price&lt;/span>, &lt;span style="color:#a6e22e">quantity&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> }],
&lt;span style="color:#a6e22e">mode&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#39;payment&amp;#39;&lt;/span>,
&lt;span style="color:#75715e">// Do not rely on the redirect to the successUrl for fulfilling
&lt;/span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// purchases, customers may not always reach the success_url after
&lt;/span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// a successful payment.
&lt;/span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Instead use one of the strategies described in
&lt;/span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// https://stripe.com/docs/payments/checkout/fulfillment
&lt;/span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">successUrl&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">DOMAIN&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#e6db74">&amp;#39;success&amp;#39;&lt;/span>,
&lt;span style="color:#a6e22e">cancelUrl&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">DOMAIN&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#e6db74">&amp;#39;canceled&amp;#39;&lt;/span>,
})
.&lt;span style="color:#a6e22e">then&lt;/span>(&lt;span style="color:#66d9ef">function&lt;/span> (&lt;span style="color:#a6e22e">result&lt;/span>) {
&lt;span style="color:#66d9ef">if&lt;/span> (&lt;span style="color:#a6e22e">result&lt;/span>.&lt;span style="color:#a6e22e">error&lt;/span>) {
&lt;span style="color:#75715e">// If `redirectToCheckout` fails due to a browser or network
&lt;/span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// error, display the localized error message to your customer.
&lt;/span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">displayError&lt;/span> &lt;span style="color:#f92672">=&lt;/span> document.&lt;span style="color:#a6e22e">getElementById&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;error-message&amp;#39;&lt;/span>);
&lt;span style="color:#a6e22e">displayError&lt;/span>.&lt;span style="color:#a6e22e">textContent&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">result&lt;/span>.&lt;span style="color:#a6e22e">error&lt;/span>.&lt;span style="color:#a6e22e">message&lt;/span>;
}
});
});
})();
&amp;lt;/&lt;span style="color:#f92672">script&lt;/span>&amp;gt;
&lt;/code>&lt;/pre>&lt;/div>&lt;p>That snippet and the button were all that I needed. Stripe will also provide testing keys for both price and API. So you can test with that first to make sure it is working.&lt;/p>
&lt;p>After I set that up, I can click my beer can and end up at a page like below.&lt;/p>
&lt;p>&lt;img src="beermoney-checkout.png" alt="beer can checkout">&lt;/p>
&lt;p>ðŸ’°ðŸ’°ðŸ’° &lt;strong>Profit&lt;/strong> ðŸ’°ðŸ’°ðŸ’°&lt;/p>
&lt;h2 id="other-cases">Other Cases&lt;/h2>
&lt;p>This works pretty well if someone would only buy one item at a time. You could probably make a cart entirely on the frontend, keeping track of items a user wants and then when a user clicks &lt;code>checkout&lt;/code>, you would send multiple &lt;code>lineItems&lt;/code> in the Stripe Redirect.&lt;/p>
&lt;p>This may not be good enough for a real store, but it's pretty convenient to have a fully client-side storefront.&lt;/p></description></item><item><title>Test HTML in GitHub Actions</title><link>https://www.joshkasuboski.com/posts/htmltest-githubactions/</link><pubDate>Wed, 07 Oct 2020 16:08:40 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/htmltest-githubactions/</guid><description>&lt;p>I want to start adding tests for this site so that I don't break it ðŸ˜¢. &lt;code>htmltest&lt;/code> is first.&lt;/p>
&lt;h2 id="setup-htmltest">Setup htmltest&lt;/h2>
&lt;p>I went with &lt;a href="https://github.com/wjdp/htmltest">htmltest&lt;/a> mainly because it's just a go binary. I've seen other options in the past that required a Ruby environment and generally don't like dealing with that.&lt;/p>
&lt;p>Installing &lt;code>htmltest&lt;/code> into the current directory can be as simple as &lt;code>curl https://htmltest.wjdp.uk | bash&lt;/code>. I installed it system-wide but ðŸ¤·â€â™‚ï¸.&lt;/p>
&lt;p>&lt;code>htmltest&lt;/code> operates on HTML files&amp;hellip;whereas my site is set up with &lt;a href="https://gohugo.io/">Hugo&lt;/a> so is largely Markdown. I have to generate my site first using &lt;code>hugo&lt;/code> and then run &lt;code>htmltest&lt;/code> on the &lt;code>public&lt;/code> folder. Fortunately, htmltest has a config file that lets you set the directory. This way testing my site (once it's rendered) is just a matter of running &lt;code>htmltest&lt;/code>.&lt;/p>
&lt;p>The first time running htmltest my site had a number of issues. It found a broken link to an svg I had removed, my LinkedIn link was incorrect, and a number of missing &lt;code>alt&lt;/code> tags. There were also a couple other issues that I ended up having ignored. I have some &lt;a href="https://indieweb.org/">indieweb&lt;/a> tags that didn't respond kindly to a &lt;code>GET&lt;/code> request and one site I linked out to kept timing out in the test.&lt;/p>
&lt;h2 id="add-to-github-actions">Add to GitHub Actions&lt;/h2>
&lt;p>I installed htmltest in my pipeline using the magic &lt;code>curl&lt;/code> script. This way it always gets the latest version and I didn't have to write anything. htmltest will cache the results of the remote links check so the workflow will cache the &lt;code>tmp/.htmltest&lt;/code> folder to help keep it quick. Overall this adds six seconds to my pipeline.&lt;/p>
&lt;p>This is all that was needed to add to the workflow yaml. It assumes htmltest is configured with &lt;code>.htmltest.yml&lt;/code>. You can find my &lt;a href="https://github.com/kasuboski/personal-site/blob/master/.htmltest.yml">config&lt;/a> in the repo.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">- uses: actions/cache@v2
with:
path: tmp/.htmltest
key: ${{ runner.os }}-htmltest
- name: HTML Test
run: &lt;span style="color:#e6db74">|
&lt;/span>&lt;span style="color:#e6db74"> &lt;/span>&lt;span style="color:#e6db74"> &lt;/span>&lt;span style="color:#e6db74">curl https://htmltest.wjdp.uk | bash&lt;/span>
bin/htmltest
&lt;/code>&lt;/pre>&lt;/div>&lt;figure>&lt;a href="actions-run.png">
&lt;img src="actions-run.png"
alt="GitHub actions run"/> &lt;/a>
&lt;/figure>
&lt;p>I did have to configure it not to fail on external links as it was failing every so often. Every change to my site now gets tested to ensure the links are valid.&lt;/p>
&lt;h2 id="next-steps-">Next Steps ðŸ¦¶&lt;/h2>
&lt;p>I want to add more tests. I think the first might be a spellchecker. I write these posts in VS Code which will highlight misspellings, but I don't always notice. I've used &lt;a href="https://github.com/errata-ai/vale">vale&lt;/a> before and it will even recommend some grammar changes.&lt;/p>
&lt;p>I also want to test the Lighthouse score and make sure it doesn't get worse. This GitHub action seems nice &lt;a href="https://github.com/marketplace/actions/lighthouse-check">lighthouse-check&lt;/a>. It could generate an HTML report like below.&lt;/p>
&lt;figure>&lt;a href="lighthouse-report.png">
&lt;img src="lighthouse-report.png"
alt="lighthouse report"/> &lt;/a>
&lt;/figure></description></item><item><title>Scan Container Images Found in Files</title><link>https://www.joshkasuboski.com/posts/scan-images-in-files/</link><pubDate>Thu, 01 Oct 2020 11:29:05 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/scan-images-in-files/</guid><description>&lt;p>I previously had been scanning images ad-hoc using Trivy. Now I can scan everything in my GitOps repo.&lt;/p>
&lt;p>The previous post about Trivy is &lt;a href="https://www.joshkasuboski.com/posts/image-scanning-trivy/">here&lt;/a>.&lt;/p>
&lt;h2 id="finding-the-images-to-scan">Finding the Images to Scan&lt;/h2>
&lt;p>Since I track all of my deployments in git at &lt;a href="https://github.com/kasuboski/k8s-gitops">kasuboski/k8s-gitops&lt;/a>, I can find all of the images from the files in that repo. For the simple case of just finding it from Kubernetes objects, I can use the command below which finds the images, removes quotes, and makes sure it's unique. Magic ðŸ§™â€â™‚ï¸.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">ack --type yaml &lt;span style="color:#e6db74">&amp;#39;image: (.*)&amp;#39;&lt;/span> --output &lt;span style="color:#e6db74">&amp;#39;$1&amp;#39;&lt;/span> -h --nobreak | tr -d &lt;span style="color:#ae81ff">\&amp;#34;&lt;/span>&lt;span style="color:#ae81ff">\&amp;#39;&lt;/span> | sort -u
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This outputs everything coming after &amp;ldquo;image: &amp;ldquo;. This works for Kubernetes resources and it also picked up some values in a helm values file. I render manifests using kustomize, which can change images and pull remotely. This means I should really be running this command on the rendered manifests.&lt;/p>
&lt;p>I've been considering rendering everything in git instead of only on the cluster (in ArgoCD). This would capture the exact yaml that was applied and make this analysis easier.&lt;/p>
&lt;h2 id="scanning-with-trivy">Scanning with Trivy&lt;/h2>
&lt;p>You can use &lt;code>xargs&lt;/code> to run a scan for each image. I did this initially just piping to &lt;code>trivy image&lt;/code>, but immediately hit the GitHub rate-limit. I thought trivy would reuse the database cache, but that didn't seem to be happening. They do let you pass a &lt;a href="https://github.com/aquasecurity/trivy#github-rate-limiting">GitHub Token&lt;/a> to increase the limit, but that seemed like more effort.&lt;/p>
&lt;p>Instead, I decided to use the client server mode of trivy. This let's you run a server that executes the scans and your client communicates with it.&lt;/p>
&lt;p>To start the server:&lt;/p>
&lt;pre>&lt;code>trivy server
&lt;/code>&lt;/pre>&lt;p>Then (in another terminal) run a &lt;code>trivy client&lt;/code> for each image:&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">ack --type yaml &lt;span style="color:#e6db74">&amp;#39;image: (.*)&amp;#39;&lt;/span> --output &lt;span style="color:#e6db74">&amp;#39;$1&amp;#39;&lt;/span> -h --nobreak | tr -d &lt;span style="color:#ae81ff">\&amp;#34;&lt;/span>&lt;span style="color:#ae81ff">\&amp;#39;&lt;/span> | sort -u | xargs -L1 trivy client --severity HIGH,CRITICAL --ignore-unfixed
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This gives output like below:
&lt;figure>&lt;a href="results.png">
&lt;img src="results.png"
alt="Trivy Results"/> &lt;/a>
&lt;/figure>
&lt;/p>
&lt;p>It did fail to analyze an image that only had an ARM manifest (I ran this on my amd64 desktop)&lt;/p>
&lt;h2 id="next-steps-">Next Steps ðŸ¦¶&lt;/h2>
&lt;p>I would like to run this repeatedly as a GitHub Action on the repo itself. It might be easier to just use a GitHub Token for the rate-limit in that case. I also want to analyze the rendered manifests.&lt;/p>
&lt;p>I have been considering setting up my own registry to mirror my images such as &lt;a href="https://goharbor.io/">Harbor&lt;/a>. Harbor can run periodic Trivy scans by itself and make the results available. There is also a an experimental &lt;a href="https://github.com/aquasecurity/trivy-enforcer">trivy-enforcer&lt;/a> project that runs in the cluster and can enforce that all images running in your cluster are secure.&lt;/p>
&lt;p>This combination of static analysis and runtime enforcement should make it pretty tough to have a known bad image running.&lt;/p></description></item><item><title>Container Image Scanning with Trivy</title><link>https://www.joshkasuboski.com/posts/image-scanning-trivy/</link><pubDate>Fri, 25 Sep 2020 11:01:50 -0600</pubDate><guid>https://www.joshkasuboski.com/posts/image-scanning-trivy/</guid><description>&lt;p>I wanted to have some peace of mind when running random container images. Trivy let's me scan them for common vulnerabilities.&lt;/p>
&lt;h2 id="installing-trivy">Installing Trivy&lt;/h2>
&lt;p>You can find the Trivy repo on GitHub at &lt;a href="https://github.com/aquasecurity/trivy">aquasecurity/trivy&lt;/a>. Installing with Homebrew is just &lt;code>brew install aquasecurity/trivy/trivy&lt;/code>. Trivy is written in Golang so you just need to get the binary. They also have a magic script you can use&lt;/p>
&lt;p>&lt;code>curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/master/contrib/install.sh | sh -s -- -b /usr/local/bin&lt;/code>&lt;/p>
&lt;p>Once you have &lt;code>trivy&lt;/code> in your &lt;code>$PATH&lt;/code>, you can run &lt;code>trivy&lt;/code> and see the options. Trivy can do a number of scans: a remote image, local filesystem, or a remote repository.&lt;/p>
&lt;p>The various options make it easy to scan code repos, images before they are pushed, and third-party images you want to use.&lt;/p>
&lt;h2 id="scanning-an-image">Scanning an image&lt;/h2>
&lt;p>I use &lt;a href="https://argoproj.github.io/projects/argo-cd">ArgoCD&lt;/a> and had to use an image other than the official one since I wanted multi-arch support. This saved me from having to build it myself which I've done in the &lt;a href="https://www.joshkasuboski.com/posts/build-multiarch-image/">past&lt;/a>.&lt;/p>
&lt;p>I wanted to scan this image for vulnerabilities (the build is open source, but you never really know). With Trivy this is as easy as &lt;code>trivy image alinbalutoiu/argocd:v1.7.1&lt;/code>. It will download (and cache) the vulnerability database and then pull and scan the image. It then outputs a nice table of vulnerabilities as seen below. You can also filter by severity and ignore unfixed.&lt;/p>
&lt;figure>&lt;a href="trivy-results.png">
&lt;img src="trivy-results.png"
alt="Trivy Results"/> &lt;/a>
&lt;/figure>
&lt;h2 id="next-stop-ci">Next Stop CI&lt;/h2>
&lt;p>This is great for testing an image ad-hoc, but I want to add this to my &lt;a href="https://github.com/kasuboski/k8s-gitops">gitops repo&lt;/a> so that all images are scanned periodically. There is already a Trivy GitHub Action, but I think it's intended more for images you are building.&lt;/p>
&lt;p>I also want to run something in my cluster that will periodically check all images that are running. Something like &lt;a href="https://github.com/aquasecurity/starboard">starboard&lt;/a> could be the beginning of that.&lt;/p></description></item><item><title>Serve a JSON API with GitHub</title><link>https://www.joshkasuboski.com/posts/stats-from-github-file/</link><pubDate>Thu, 17 Sep 2020 13:18:28 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/stats-from-github-file/</guid><description>&lt;p>I wanted to add stats to a site, but I already capture them in a GitHub Repo. Let's just pull from there.&lt;/p>
&lt;h2 id="the-stats-repo">The Stats Repo&lt;/h2>
&lt;p>I made a repo that pulls in stats (&lt;a href="https://github.com/kasuboski/stats">kasuboski/stats&lt;/a>). It uses a GitHub Action I made for a &lt;a href="https://dev.to/kasuboski/dev-to-article-stats-github-action-30n4">Dev.to&lt;/a> Hackathon that pulls post stats from Dev.to.&lt;/p>
&lt;p>The repo gets periodically updated with a &lt;code>stats/dev-to.json&lt;/code> file. GitHub lets you browse the contents of files at &lt;code>raw.githubusercontent.com&lt;/code>. In my case, this file is at &lt;a href="https://raw.githubusercontent.com/kasuboski/stats/main/stats/dev-to.json">https://raw.githubusercontent.com/kasuboski/stats/main/stats/dev-to.json&lt;/a>.&lt;/p>
&lt;h2 id="fetching-the-data">Fetching the data&lt;/h2>
&lt;p>I have a &lt;a href="https://joshcorp.co">landing page&lt;/a> served from my &lt;a href="https://www.joshkasuboski.com/posts/home-k8s-raspberry-update/">Raspberry Pi Cluster&lt;/a>. It was a placeholder with a link to my &lt;a href="https://www.joshkasuboski.com">personal site&lt;/a>. Now it also shows stats from my &lt;a href="https://dev.to/kasuboski">Dev.to posts&lt;/a>.&lt;/p>
&lt;figure>&lt;a href="landing-page-stats.png">
&lt;img src="landing-page-stats.png"
alt="Stats on the landing page"/> &lt;/a>
&lt;/figure>
&lt;p>The landing page itself is just vanilla HTML/CSS/JS. It uses &lt;a href="https://andybrewer.github.io/mvp/">mvp.css&lt;/a> to get quick styles. The repo is &lt;a href="https://github.com/kasuboski/joshcorp-site">kasuboski/joshcorp-site&lt;/a>. The javascript needed to add the stats is below. It's just in a &lt;code>script&lt;/code> tag in the body.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-javascript" data-lang="javascript">&lt;span style="color:#66d9ef">function&lt;/span> &lt;span style="color:#a6e22e">getStats&lt;/span>() {
&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">stats&lt;/span> &lt;span style="color:#f92672">=&lt;/span> document.&lt;span style="color:#a6e22e">querySelector&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;#stats&amp;#39;&lt;/span>);
&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">reactions&lt;/span> &lt;span style="color:#f92672">=&lt;/span> document.&lt;span style="color:#a6e22e">querySelector&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;#reactions-value&amp;#39;&lt;/span>);
&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">views&lt;/span> &lt;span style="color:#f92672">=&lt;/span> document.&lt;span style="color:#a6e22e">querySelector&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;#views-value&amp;#39;&lt;/span>);
&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">url&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;https://raw.githubusercontent.com/kasuboski/stats/main/stats/dev-to.json&amp;#39;&lt;/span>;
&lt;span style="color:#a6e22e">fetch&lt;/span>(&lt;span style="color:#a6e22e">url&lt;/span>)
.&lt;span style="color:#a6e22e">then&lt;/span>(&lt;span style="color:#a6e22e">res&lt;/span> =&amp;gt; &lt;span style="color:#a6e22e">res&lt;/span>.&lt;span style="color:#a6e22e">json&lt;/span>())
.&lt;span style="color:#a6e22e">then&lt;/span>(&lt;span style="color:#a6e22e">data&lt;/span> =&amp;gt; {
&lt;span style="color:#a6e22e">console&lt;/span>.&lt;span style="color:#a6e22e">log&lt;/span>(&lt;span style="color:#a6e22e">data&lt;/span>);
&lt;span style="color:#a6e22e">reactions&lt;/span>.&lt;span style="color:#a6e22e">innerText&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">data&lt;/span>.&lt;span style="color:#a6e22e">public_reactions_count&lt;/span>;
&lt;span style="color:#a6e22e">views&lt;/span>.&lt;span style="color:#a6e22e">innerText&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">data&lt;/span>.&lt;span style="color:#a6e22e">page_views_count&lt;/span>;
&lt;span style="color:#a6e22e">stats&lt;/span>.&lt;span style="color:#a6e22e">style&lt;/span>.&lt;span style="color:#a6e22e">display&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;block&amp;#34;&lt;/span>;
})
.&lt;span style="color:#66d9ef">catch&lt;/span>(&lt;span style="color:#a6e22e">err&lt;/span> =&amp;gt; {
&lt;span style="color:#a6e22e">console&lt;/span>.&lt;span style="color:#a6e22e">error&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;Error fetching stats: &amp;#39;&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span>);
})
}
window.&lt;span style="color:#a6e22e">onload&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">getStats&lt;/span>;
&lt;/code>&lt;/pre>&lt;/div>&lt;p>I'm sure this probably isn't something GitHub exactly recommends&amp;hellip; but as long as you don't have too much traffic it should be fine.&lt;/p></description></item><item><title>Getting Push Notifications for Everything with Pushover</title><link>https://www.joshkasuboski.com/posts/pushover-notifications/</link><pubDate>Thu, 10 Sep 2020 13:52:17 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/pushover-notifications/</guid><description>&lt;p>I wanted to know when my personal site was deployed and decided to get push notifications.&lt;/p>
&lt;p>I make changes to my site locally and then push to GitHub so it is automatically deployed using GitHub Pages. I went over that in &lt;a href="https://www.joshkasuboski.com/posts/deploy-site-github-actions/">Deploying this site with GitHub Actions&lt;/a>. After I pushed though, I would often have to keep checking to see when it should be available. Now I get notified.&lt;/p>
&lt;h2 id="setting-up-pushover">Setting up Pushover&lt;/h2>
&lt;p>In order to get push notifications from multiple apps, I signed up for &lt;a href="https://pushover.net/">Pushover&lt;/a>. This lets me install just the Pushover app and all notifications will come through there.&lt;/p>
&lt;p>I signed up for an account and downloaded the app. I was immediately able to manually send notifications. My original intention was to set this up for GitHub Actions though.&lt;/p>
&lt;p>I created a Pushover app to get an API Token. Using this token and your user key, you can send notifications with an API call.&lt;/p>
&lt;h2 id="adding-to-github-actions">Adding to GitHub Actions&lt;/h2>
&lt;p>To get the status of my &lt;a href="https://github.com/kasuboski/personal-site/blob/master/.github/workflows/gh-pages.yaml">personal-site workflow&lt;/a>, I just need to curl the endpoint with my token and user key.&lt;/p>
&lt;p>I added both to my repo secrets and then could add the below step at the end of my workflow.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">- name: Notify
if: always()
uses: wei/curl@v1
with:
args: -X POST -F &lt;span style="color:#e6db74">&amp;#39;token=${{ secrets.PUSHOVER_TOKEN }}&amp;#39;&lt;/span> -F &lt;span style="color:#e6db74">&amp;#39;user=${{ secrets.PUSHOVER_USER }}&amp;#39;&lt;/span> -F &lt;span style="color:#e6db74">&amp;#39;message=Personal Site Pipeline ${{ job.status }}&amp;#39;&lt;/span> https://api.pushover.net/&lt;span style="color:#ae81ff">1&lt;/span>/messages.json
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="and-more">And more&lt;/h2>
&lt;p>Pushover has a number of &lt;a href="https://pushover.net/apps">integrations&lt;/a>. I have it setup to send me &lt;a href="https://uptimerobot.com/">UptimeRobot&lt;/a> alerts and also use it for Radarr. UptimeRobot just required me to add my user key and I immediately got a test message.&lt;/p>
&lt;p>No longer will I be checking the status of anything ðŸ˜‰.&lt;/p></description></item><item><title>Connect Your Home to the Cloud with Tailscale</title><link>https://www.joshkasuboski.com/posts/connect-home-cloud-tailscale/</link><pubDate>Mon, 24 Aug 2020 11:05:17 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/connect-home-cloud-tailscale/</guid><description>&lt;p>I set up my Raspberry Pi cluster to be accessible from the internet without configuring a port-forward on my router.&lt;/p>
&lt;h2 id="tailscale">Tailscale&lt;/h2>
&lt;p>&lt;a href="https://tailscale.com/">Tailscale&lt;/a> will create a private network using &lt;a href="https://www.wireguard.com/">Wireguard&lt;/a>. Wireguard isn't really that difficult to configure on its own, but you do have to manually generate and distribute keys. Tailscale will take care of that for you and they also have some &lt;a href="https://tailscale.com/blog/how-nat-traversal-works/">fallbacks&lt;/a> for difficult networks. It doesn't look like any of my nodes are using a fallback option based on the dashboard.&lt;/p>
&lt;p>Setting up Tailscale is as easy as installing it and running &lt;code>tailscale up&lt;/code>. Until recently, this required you to login interactively. Tailscale now supports &lt;a href="https://tailscale.com/kb/1068/acl-tags#pre-authenticated-keys">pre-authenticated&lt;/a> keys which means you can automate the setup.&lt;/p>
&lt;h2 id="installing-on-raspberry-pis">Installing on Raspberry PIs&lt;/h2>
&lt;p>I made &lt;a href="https://github.com/kasuboski/tailscale-install">kasuboski/tailscale-install&lt;/a> to automate the installation and start of Tailscale on Raspberry PIs. I plan to expand it to work on more varied platforms in the future.&lt;/p>
&lt;p>It's a &lt;a href="https://pyinfra.com/">PyInfra deploy&lt;/a> that basically just adds the package and runs &lt;code>tailscale up&lt;/code> with a key sourced from the environment. I was able to add my Raspberry Pi cluster to the network in around 5 minutes, using this.&lt;/p>
&lt;h2 id="exposing-to-the-internet">Exposing to the internet&lt;/h2>
&lt;p>My cluster ingress is now slightly different than described &lt;a href="https://www.joshkasuboski.com/posts/home-k8s-raspberry-update/">here&lt;/a>. Traffic from the Linode now goes directly to the Kubernetes nodes on the port exposed by the nginx-ingress controller. This just removes the extra hop that was initially an internal haproxy running on a different Raspberry Pi.&lt;/p>
&lt;figure>
&lt;img src="tailscale-diagram.png"
alt="Network Diagram"/>
&lt;/figure></description></item><item><title>Generate Your Resume with GitHub Actions</title><link>https://www.joshkasuboski.com/posts/generate-resume-gh-actions/</link><pubDate>Tue, 18 Aug 2020 10:39:19 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/generate-resume-gh-actions/</guid><description>&lt;p>I got tired of editing my resume in HTML and then printing a PDF from Chrome. I now use GitHub Actions and a json resume to generate both formats.&lt;/p>
&lt;h2 id="defining-a-json-resume">Defining a JSON Resume&lt;/h2>
&lt;p>There's a jsonresume project that defines a &lt;a href="https://json-schema.org/">JSON Schema&lt;/a> for a resume. You can find the schema repo &lt;a href="https://github.com/jsonresume/resume-schema">here&lt;/a>.&lt;/p>
&lt;p>I wanted to define my resume like this so I could easily generate multiple formats of it. My file can be found &lt;a href="https://github.com/kasuboski/resume/blob/master/resume.json">here&lt;/a>. I used to use the &lt;a href="https://github.com/jsonresume/resume-cli">resume-cli&lt;/a> project to generate the html and pdf version of my resume, but it stopped working for me awhile ago.&lt;/p>
&lt;p>I decided to convert the theme I was using to a Go template instead. That template is &lt;a href="https://github.com/kasuboski/resume/blob/master/hack/resume.html.tmpl">here&lt;/a>. It treats &lt;code>resume.json&lt;/code> as a map so the template just directly accesses the properties.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-golang" data-lang="golang">&lt;span style="color:#75715e">// hack/template.go
&lt;/span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">tmpl&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">template&lt;/span>.&lt;span style="color:#a6e22e">Must&lt;/span>(&lt;span style="color:#a6e22e">template&lt;/span>.&lt;span style="color:#a6e22e">ParseFiles&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;hack/resume.html.tmpl&amp;#34;&lt;/span>))
&lt;span style="color:#a6e22e">bs&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">ioutil&lt;/span>.&lt;span style="color:#a6e22e">ReadFile&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;resume.json&amp;#34;&lt;/span>)
&lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;span style="color:#a6e22e">log&lt;/span>.&lt;span style="color:#a6e22e">Fatalf&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;couldn&amp;#39;t read resume.json: %v&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span>)
}
&lt;span style="color:#a6e22e">f&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">os&lt;/span>.&lt;span style="color:#a6e22e">Create&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;resume.html&amp;#34;&lt;/span>)
&lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;span style="color:#a6e22e">log&lt;/span>.&lt;span style="color:#a6e22e">Fatalf&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;couldn&amp;#39;t open out.html: %v&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span>)
}
&lt;span style="color:#66d9ef">defer&lt;/span> &lt;span style="color:#a6e22e">f&lt;/span>.&lt;span style="color:#a6e22e">Close&lt;/span>()
&lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">params&lt;/span> &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">interface&lt;/span>{}
&lt;span style="color:#a6e22e">err&lt;/span> = &lt;span style="color:#a6e22e">json&lt;/span>.&lt;span style="color:#a6e22e">Unmarshal&lt;/span>(&lt;span style="color:#a6e22e">bs&lt;/span>, &lt;span style="color:#f92672">&amp;amp;&lt;/span>&lt;span style="color:#a6e22e">params&lt;/span>)
&lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;span style="color:#a6e22e">log&lt;/span>.&lt;span style="color:#a6e22e">Fatalf&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;unable to unmarshal json: %v&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span>)
}
&lt;span style="color:#a6e22e">tmpl&lt;/span>.&lt;span style="color:#a6e22e">Execute&lt;/span>(&lt;span style="color:#a6e22e">f&lt;/span>, &lt;span style="color:#a6e22e">params&lt;/span>)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now I can get the HTML version of my resume with &lt;code>go run hack/template.go&lt;/code> which will output a &lt;code>resume.html&lt;/code> file. I could then open this in chrome and print it from there, but that's so much effort ðŸ˜‰.&lt;/p>
&lt;h2 id="generating-multiple-formats-in-github-actions">Generating Multiple Formats in GitHub Actions&lt;/h2>
&lt;p>I already had a GitHub Actions workflow that would sync my resume from the resume repo to my personal-site repo, but I was manually pushing the HTML and PDF files to the resume repo. Now that I have the HTML generation working again, I decided to automate the entire process. That includes creating GitHub Releases.&lt;/p>
&lt;p>My resume repo &lt;a href="https://github.com/kasuboski/resume">kasuboski/resume&lt;/a> now has a &lt;code>create-release&lt;/code> workflow. Pushing changes to &lt;code>resume.json&lt;/code> or tagging a commit will now do the below.&lt;/p>
&lt;ul>
&lt;li>Generate the html with &lt;code>go run hack/template.go&lt;/code>&lt;/li>
&lt;li>Generate a PDF using &lt;a href="https://github.com/fifsky/html-to-pdf-action">fifsky/html-to-pdf-action&lt;/a>&lt;/li>
&lt;li>Add the html and pdf as a build artifact (so you can manually inspect before releasing)&lt;/li>
&lt;li>Create a release for a tag with the files&lt;/li>
&lt;li>Update my personal site with the new files if they've changed&lt;/li>
&lt;/ul>
&lt;p>Updating my resume now involves just updating &lt;code>resume.json&lt;/code> and the files are generated and pushed to my site.&lt;/p>
&lt;h2 id="future">Future&lt;/h2>
&lt;p>I'd like to start managing more things like this. Maybe pushing to my LinkedIn profile or updating a GitHub profile README. It'll be like &lt;a href="https://www.weave.works/technologies/gitops/">GitOps&lt;/a> but more Git&amp;hellip;personal info.&lt;/p></description></item><item><title>I Mounted My PC - Server Rack Update</title><link>https://www.joshkasuboski.com/posts/server-rack-2/</link><pubDate>Sun, 16 Aug 2020 13:33:52 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/server-rack-2/</guid><description>&lt;p>I got my desktop mounted in the rack and added wheels. It's really moving ðŸ˜&lt;/p>
&lt;p>You can see the previous server rack start &lt;a href="https://www.joshkasuboski.com/posts/server-rack-1/">here&lt;/a>, where I had just gotten it.&lt;/p>
&lt;h2 id="making-it-mobile">Making it mobile&lt;/h2>
&lt;p>Moving the rack back and forth was pretty rough. It was definitely scratching the concrete and since I was planning to put my desktop in, the rack was about to get much heavier.&lt;/p>
&lt;p>The rack I purchased came with a piece of steel that sticks out and has 3/4&amp;rdquo; holes drilled in. I wanted to put casters there, but 3/4&amp;rdquo; stem casters don't seem to be a thing. I ended up getting 1/2&amp;rdquo; stem casters and some washers to make it fit the hole. It seems sturdy enough (I was able to stand on it without incident).&lt;/p>
&lt;figure>&lt;a href="wheel-holes.jpg">
&lt;img src="wheel-holes.jpg"
alt="Steel piece to put the wheels" width="300px"/> &lt;/a>
&lt;/figure>
&lt;figure>&lt;a href="caster.jpg">
&lt;img src="caster.jpg"
alt="Wheel attached with washer" width="300px"/> &lt;/a>
&lt;/figure>
&lt;p>Now that the rack is able to be rolled out I wanted to make sure I wouldn't always have to disconnect the internet when moving it. I made sure to plug the modem and router into a power strip that has enough slack to move with the rack.&lt;/p>
&lt;p>Everything else plugs into a rack mountable &lt;a href="https://www.amazon.com/gp/product/B0781WS2M5">power strip&lt;/a> so nothing needs to be unplugged when rolling it forward.&lt;/p>
&lt;h2 id="adding-my-pc">Adding my PC&lt;/h2>
&lt;p>I moved my PC from a normal desktop case to a &lt;a href="https://www.newegg.com/black-rosewill-rsv-r4000/p/N82E16811147154?Item=N82E16811147154">4U Rosewill Server Case&lt;/a>. I was able to fit everything in without issue, including my graphics card and Blu-Ray drive. I ended up replacing all of the fans since the included ones had molex connectors with the fans running at full speed all the time.&lt;/p>
&lt;figure>&lt;a href="open-case.jpg">
&lt;img src="open-case.jpg"
alt="PC installed in server case" width="300px"/> &lt;/a>
&lt;/figure>
&lt;p>In the front, there are 8 tool-less hard drive bays. I am currently using none of them as I just have two SSDs sitting in there. I am planning to eventually add storage, but may get a separate enclosure.&lt;/p>
&lt;p>I installed a rack mount &lt;a href="https://www.amazon.com/gp/product/B00XXDJASY">shelf rail&lt;/a> for the case to sit on. Unfortunately, that meant I had to take off the door of the Rosewill case since it didn't fit within the mount of the rails. I hope to be able to make something slimmer so it'll fit back on.&lt;/p>
&lt;figure>&lt;a href="assembled-rack.jpg">
&lt;img src="assembled-rack.jpg"
alt="Assembled server rack" width="300px"/> &lt;/a>
&lt;/figure>
&lt;p>The final rack looks a little better and is certainly more functional. It freed up some space under my desk where I now have a 12TB external hard drive.&lt;/p>
&lt;p>I still want to mount the &lt;a href="https://www.joshkasuboski.com/posts/home-k8s-raspberry-update/">Raspberry Pi cluster&lt;/a> in the rack. I bought this &lt;a href="https://www.musicstore.de/de_DE/EUR/DAP-2-HE-Rackblende-f-Modulsystem-10-Segmente-MP-1/art-PAH0017160-000;pgid=WBtg67.syLdSRpoV6L_EAtys0000YDPT2oVh">bracket&lt;/a> in order to do something like these &lt;a href="https://www.kaibader.de/3d-printed-raspberry-pi-rack-mount-with-heat-sink-passive-cooling/">mounts&lt;/a>, but it still hasn't arrived. I may have to call and see what happened (brushing up on my German I guess).&lt;/p></description></item><item><title>Managing the Cluster with ArgoCD</title><link>https://www.joshkasuboski.com/posts/argocd-managed/</link><pubDate>Sun, 26 Jul 2020 14:24:26 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/argocd-managed/</guid><description>&lt;p>I had to manually apply changes to my cluster, but now a lot of it is controlled from git thanks to ArgoCD.&lt;/p>
&lt;h2 id="managed-with-a-cluster-repo">Managed with a cluster repo&lt;/h2>
&lt;p>The cluster state is kept in &lt;a href="https://github.com/kasuboski/k8s-gitops">kasuboski/k8s-gitops&lt;/a>. Each folder is a different function for the cluster. The gitops folder is special. It has the &lt;a href="https://argoproj.github.io/projects/argo-cd">ArgoCD&lt;/a> manifests and the apps folder.&lt;/p>
&lt;p>ArgoCD will sync manifests from a git repo to the cluster. It continuously watches to make sure the desired state (from git) matches the observed state (in the cluster). You configure ArgoCD using CRDs. Argo has Apps and Projects. A Project configures access for Apps and an App represents a repo that deploys Kubernetes objects.&lt;/p>
&lt;p>You can see my apps at &lt;a href="https://github.com/kasuboski/k8s-gitops/tree/master/gitops/apps">gitops/apps&lt;/a>. I use an app of apps &lt;a href="https://argoproj.github.io/argo-cd/operator-manual/cluster-bootstrapping/#app-of-apps-pattern">pattern&lt;/a> to deploy. I apply this root &amp;ldquo;apps&amp;rdquo; app that then deploys the other apps for the cluster.&lt;/p>
&lt;p>ArgoCD also comes with a dashboard that shows the apps and their status. You can also get a badge to put in the repo to show overall status.&lt;/p>
&lt;p>&lt;img src="https://argocd.joshcorp.co/api/badge?name=apps&amp;amp;revision=true" alt="argocd-badge">&lt;/p>
&lt;figure>&lt;a href="argo-dashboard.png">
&lt;img src="argo-dashboard.png"
alt="The argo dashboard shows apps" width="800px"/> &lt;/a>
&lt;/figure>
&lt;h2 id="but-why">But why&lt;/h2>
&lt;p>Managing Kubernetes like this (once everything is automated) allows me to stand up a cluster with the same state simply by installing ArgoCD and applying an ArgoCD App.&lt;/p>
&lt;p>It also keeps me from accidentally messing with the cluster since even if I delete a Deployment ArgoCD will reapply it.&lt;/p></description></item><item><title>GitOpsing the cluster</title><link>https://www.joshkasuboski.com/posts/gitops-cluster-1/</link><pubDate>Sun, 12 Jul 2020 18:40:37 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/gitops-cluster-1/</guid><description>&lt;p>I kept track of how I set up my &lt;a href="https://www.joshkasuboski.com/posts/home-k8s-raspberry-update/">Raspberry Pi cluster&lt;/a> along the way, but hadn't committed it to git. Today that changed.&lt;/p>
&lt;h2 id="gitops-and-the-repo">GitOps and the repo&lt;/h2>
&lt;p>If you're not familiar with GitOps, the people at weaveworks have a nice &lt;a href="https://www.weave.works/technologies/gitops/">article&lt;/a>.&lt;/p>
&lt;p>I pushed my setup to &lt;a href="https://github.com/kasuboski/k8s-gitops">kasuboski/k8s-gitops&lt;/a>. It should have everything that's deployed on my cluster.&lt;/p>
&lt;p>Each folder at the top level is basically a namespace currently. The ingress folder does contain the &lt;code>ingress-nginx&lt;/code> and &lt;code>cert-manager&lt;/code> namespaces.&lt;/p>
&lt;p>I hadn't pushed it earlier because I still haven't figured out my strategy for secrets. For now, the only secret needed is for fathom. I used git-crypt to encrypt on push and decrypt on pull. That works fine for now. There's a nice walkthrough &lt;a href="https://buddy.works/guides/git-crypt">here&lt;/a>.&lt;/p>
&lt;p>I was looking to use &lt;a href="https://secrethub.io/">secrethub.io&lt;/a>, but would need to figure out how I want to interface with it. In the past, I've made an operator similar to &lt;a href="https://github.com/godaddy/kubernetes-external-secrets">kubernetes-external-secrets&lt;/a>. That will authenticate a workload and fetch its credentials from outside the cluster. I wanted to use &lt;a href="https://spiffe.io/">SPIFFE&lt;/a> for workload identity, but it seemed &lt;a href="https://spiffe.io/spire/">Spire&lt;/a> doesn't publish ARM images.&lt;/p>
&lt;h2 id="fully-reconciling">Fully reconciling&lt;/h2>
&lt;p>My cluster is still managed manually, albeit from checked in manifests (it also &lt;a href="https://www.joshkasuboski.com/posts/k8s-auto-upgrades/">upgrades&lt;/a> automatically).&lt;/p>
&lt;p>The next step is to use a GitOps Operator. I've used &lt;a href="https://argoproj.github.io/projects/argo-cd">ArgoCD&lt;/a> before, but &lt;a href="https://fluxcd.io/">flux&lt;/a> has been seeming more lightweight. It may come down to which one supports ARM better.&lt;/p>
&lt;p>I also want to make the yaml easier to manage. First, using &lt;a href="https://github.com/kubernetes-sigs/kustomize">kustomize&lt;/a> to tie it all together and then exploring &lt;a href="https://github.com/jkcfg/jk">jk&lt;/a> to make templates in Typescript.&lt;/p></description></item><item><title>Automated Upgrades for k3s</title><link>https://www.joshkasuboski.com/posts/k8s-auto-upgrades/</link><pubDate>Wed, 08 Jul 2020 18:50:17 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/k8s-auto-upgrades/</guid><description>&lt;p>My cluster was falling behind the latest k3s version. Time to upgrade.&lt;/p>
&lt;h2 id="enter-the-system-upgrade-controller">Enter the System Upgrade Controller&lt;/h2>
&lt;p>I basically just did the instructions linked &lt;a href="https://rancher.com/docs/k3s/latest/en/upgrades/automated/">here&lt;/a>.&lt;/p>
&lt;p>It deploys the system upgrade controller into its own namespace where it won't do anything yet.&lt;/p>
&lt;p>You have to give it some plans.&lt;/p>
&lt;p>It will read Plans in that same namespace and run the specified image. I used the plan linked above. Instead of tying it to a specific version I set it to the latest channel.&lt;/p>
&lt;pre>&lt;code># Server plan
apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
name: server-plan
namespace: system-upgrade
spec:
concurrency: 1
cordon: true
nodeSelector:
matchExpressions:
- key: node-role.kubernetes.io/master
operator: In
values:
- &amp;quot;true&amp;quot;
serviceAccountName: system-upgrade
upgrade:
image: rancher/k3s-upgrade
channel: https://update.k3s.io/v1-release/channels/stable
---
# Agent plan
apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
name: agent-plan
namespace: system-upgrade
spec:
concurrency: 1
cordon: true
nodeSelector:
matchExpressions:
- key: node-role.kubernetes.io/master
operator: DoesNotExist
prepare:
args:
- prepare
- server-plan
image: rancher/k3s-upgrade:v1.17.4-k3s1
serviceAccountName: system-upgrade
upgrade:
image: rancher/k3s-upgrade
channel: https://update.k3s.io/v1-release/channels/stable
&lt;/code>&lt;/pre>&lt;p>You'll notice there are actually 2 plans. One applied to the master nodes and the other the worker nodes. This lets you upgrade the master node to the new version before the workers.&lt;/p>
&lt;p>The entire process took maybe 5 minutes for my 3 node cluster. I was able to &lt;code>kubectl get nodes&lt;/code> periodically and watch the progress.&lt;/p></description></item><item><title>Server Rack Beginnings</title><link>https://www.joshkasuboski.com/posts/server-rack-1/</link><pubDate>Sun, 05 Jul 2020 13:58:33 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/server-rack-1/</guid><description>&lt;p>My network setup was starting to be a mess of a corner. The natural thing to do is to get a server rack to contain everything ðŸ˜‰&lt;/p>
&lt;h2 id="the-mess">The Mess&lt;/h2>
&lt;figure>&lt;a href="network-mess.jpg">
&lt;img src="network-mess.jpg"
alt="A Jumbled mess of cables" width="300px"/> &lt;/a>
&lt;/figure>
&lt;p>The network corner was already a mess, but adding my &lt;a href="https://www.joshkasuboski.com/posts/home-k8s-raspberry-update/">Raspberry Pi cluster&lt;/a> definitely didn't help.&lt;/p>
&lt;p>You can see the following items if you look closely:&lt;/p>
&lt;ul>
&lt;li>Linksys Router&lt;/li>
&lt;li>5-port switch for the Raspberry Pi cluster&lt;/li>
&lt;li>Raspberry Pi Cluster&lt;/li>
&lt;li>Raspberry Pi 3B+&lt;/li>
&lt;li>Hue Hub&lt;/li>
&lt;/ul>
&lt;p>Not pictured is my ISP modem and the USB power hub for the Pi cluster.&lt;/p>
&lt;h2 id="solution">Solution?&lt;/h2>
&lt;p>I have been following &lt;a href="https://www.reddit.com/r/selfhosted/">r/selfhosted&lt;/a> and &lt;a href="https://www.reddit.com/r/homelab/">r/homelab&lt;/a> for a while. It was perhaps a mistake.&lt;/p>
&lt;p>I now aspire to the setups such as &lt;a href="https://hydn.dev/homelab/">this&lt;/a> and &lt;a href="https://tynick.com/blog/06-06-2019/my-humble-homelab-with-raspberry-pi-rack/">this&lt;/a>.&lt;/p>
&lt;p>The first link has a nice monitor on top and the second has a rackmount Pi enclosure (which I have purchased the bracket for).&lt;/p>
&lt;p>I bought a &lt;a href="https://www.ebay.com/itm/15U-4-Post-Open-Frame-Server-Rack-Enclosure-19-Adjustable-Depth/151584303195?ssPageName=STRK%3AMEBIDX%3AIT&amp;amp;_trksid=p2057872.m2749.l2649">15U open enclosure rack&lt;/a>. It's maybe not the most efficient solution, but definitely feels &amp;hellip; cool.&lt;/p>
&lt;h2 id="setting-up-the-rack">Setting up the Rack&lt;/h2>
&lt;p>The rack arrived with some assembly required. It took me an hour to put it together with the help of a magnetic level.&lt;/p>
&lt;figure>&lt;a href="rack-box.jpg">
&lt;img src="rack-box.jpg"
alt="The server rack some assembly required" width="300px"/> &lt;/a>
&lt;/figure>
&lt;figure>&lt;a href="assembled-rack.jpg">
&lt;img src="assembled-rack.jpg"
alt="Assembled rack" width="300px"/> &lt;/a>
&lt;/figure>
&lt;p>I got everything into a &lt;a href="https://www.ebay.com/itm/Cantilever-Server-Shelf-Vented-Black-Shelves-Rack-Mount-19-1U-12-300mm-Deep/152062041884?ssPageName=STRK%3AMEBIDX%3AIT&amp;amp;_trksid=p2057872.m2749.l2649">shelf&lt;/a> and I'm not convinced it looks all that much better. The messy desk obviously isn't helping ðŸ˜¢.&lt;/p>
&lt;figure>&lt;a href="completed-rack.jpg">
&lt;img src="completed-rack.jpg"
alt="Completed rack" width="300px"/> &lt;/a>
&lt;/figure>
&lt;h2 id="moving-forward">Moving Forward&lt;/h2>
&lt;p>The rack may not have immediately fixed my issues, but I'm planning to either replace or augment my set up to be rackmountable.&lt;/p>
&lt;p>My first thing to rackmount is the Pi Cluster. I'm going to use something like &lt;a href="https://www.kaibader.de/3d-printed-raspberry-pi-rack-mount-with-heat-sink-passive-cooling/">these 3D printed mounts&lt;/a> to put the Pis in the rack.&lt;/p>
&lt;p>I'm also looking at getting a 4U case to transfer my desktop to and switching to a rackmounted switch and router.&lt;/p>
&lt;p>In the meantime the Pi cluster is pretty photogenic.&lt;/p>
&lt;figure>&lt;a href="artsy-pi.jpg">
&lt;img src="artsy-pi.jpg"
alt="Artsy Pi" width="400px"/> &lt;/a>
&lt;/figure></description></item><item><title>Persistent Storage with OpenEBS</title><link>https://www.joshkasuboski.com/posts/openebs-homelab/</link><pubDate>Tue, 23 Jun 2020 18:18:22 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/openebs-homelab/</guid><description>&lt;p>My cluster monitoring prometheus kept falling over because it was running out of disk space. After finally getting annoyed having to restart it, I decided it was time for persistent storage.&lt;/p>
&lt;p>I did have the &lt;a href="https://github.com/rancher/local-path-provisioner">local-path-provisioner&lt;/a> running, but I didn't feel great about using the SD card for general storage.&lt;/p>
&lt;p>I bought 2 &lt;a href="https://www.amazon.com/gp/product/B07T5XGWZY/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&amp;amp;psc=1">USB drives&lt;/a> to add to my workers. In hindsight, I probably should have gotten 3 for better redundancy.&lt;/p>
&lt;p>I decided to go with &lt;a href="https://openebs.io/">OpenEBS&lt;/a>. It works as Container Attached Storage and seemed more lightweight and flexible than other options. They also publish arm64 images which is always a plus.&lt;/p>
&lt;h2 id="prepare-the-cluster">Prepare the cluster&lt;/h2>
&lt;p>I found the OpenEBS docs a little hard to follow, but this is what I ended up doing.&lt;/p>
&lt;ul>
&lt;li>Attach your USB drives to the workers (hopefully yours are labeled)&lt;/li>
&lt;li>install iscsi on every node&lt;/li>
&lt;/ul>
&lt;pre>&lt;code>sudo apt-get update
sudo apt-get install -y open-iscsi
sudo systemctl enable --now iscsid
&lt;/code>&lt;/pre>&lt;h2 id="install-openebs">Install OpenEBS&lt;/h2>
&lt;p>I installed using the helm chart. The only changes with the values file below is basically changing all images to the arm64 version. It seems they don't have great support for a mixed architecture cluster.&lt;/p>
&lt;pre>&lt;code>kubectl create ns openebs
helm repo add openebs https://openebs.github.io/charts
helm install openebs openebs/openebs --namespace openebs --version 1.11.1 -f openebs-values.yaml
&lt;/code>&lt;/pre>&lt;pre>&lt;code>ndm:
image: 'openebs/node-disk-manager-arm64'
ndmOperator:
image: 'openebs/node-disk-operator-arm64'
webhook:
image: 'openebs/admission-server-arm64'
apiserver:
image: 'openebs/m-apiserver-arm64'
localprovisioner:
image: 'openebs/provisioner-localpv-arm64'
snapshotOperator:
controller:
image: 'openebs/snapshot-controller-arm64'
provisioner:
image: 'openebs/snapshot-provisioner-arm64'
provisioner:
image: 'openebs/openebs-k8s-provisioner-arm64'
helper:
image: 'openebs/linux-utils-arm64'
cstor:
pool:
image: 'openebs/cstor-pool-arm64'
poolMgmt:
image: 'openebs/cstor-pool-mgmt-arm64'
target:
image: 'openebs/cstor-istgt-arm64'
volumeMgmt:
image: 'openebs/cstor-volume-mgmt-arm64'
policies:
monitoring:
image: 'openebs/m-exporter-arm64'
analytics:
enabled: false
&lt;/code>&lt;/pre>&lt;p>If all is well you should see the pods in the openebs namespace as healthy. My usb drives automatically showed up as block devices as well.&lt;/p>
&lt;ul>
&lt;li>&lt;code>kubectl get pods -n openebs&lt;/code>&lt;/li>
&lt;li>&lt;code>kubectl get sc&lt;/code>&lt;/li>
&lt;li>&lt;code>kubectl get blockdevice -n openebs&lt;/code>&lt;/li>
&lt;/ul>
&lt;figure>&lt;a href="storage-class-block-devices.png">
&lt;img src="storage-class-block-devices.png"
alt="Block devices detected"/> &lt;/a>
&lt;/figure>
&lt;h2 id="configure-openebs">Configure OpenEBS&lt;/h2>
&lt;p>I differed from the quickstart a little bit here. It was a little confusing to me what I should do with only 2 devices.&lt;/p>
&lt;p>I eventually found this &lt;a href="https://github.com/openebs/openebs-docs/issues/486">issue&lt;/a> talking about configuring volumes with a single replica and made it use 2 pools instead of 1.&lt;/p>
&lt;p>The below yaml will set up a StoragePoolClaim with a maxPools of 2 (which will just use both of my nodes with a drive) and a StorageClass configured to use a single replica. I went with striped because it seemed more flexible and since each node only has 1 disk right now it didn't seem too important.&lt;/p>
&lt;pre>&lt;code>---
apiVersion: openebs.io/v1alpha1
kind: StoragePoolClaim
metadata:
name: cstor-disk
spec:
name: cstor-disk
type: disk
maxPools: 2
poolSpec:
poolType: striped
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: openebs-cstor-1-replica-disk
annotations:
openebs.io/cas-type: cstor
cas.openebs.io/config: |
- name: StoragePoolClaim
value: &amp;quot;cstor-disk&amp;quot;
- name: ReplicaCount
value: &amp;quot;1&amp;quot;
provisioner: openebs.io/provisioner-iscsi
&lt;/code>&lt;/pre>&lt;p>After this is applied, you should be able to see the claims and corresponding pods.&lt;/p>
&lt;figure>&lt;a href="claimed-storage-pool.png">
&lt;img src="claimed-storage-pool.png"
alt="Storage Pool"/> &lt;/a>
&lt;/figure>
&lt;h2 id="persistent-prometheus">Persistent Prometheus&lt;/h2>
&lt;p>I used this StorageClass for my cluster Prometheus. It took awhile for the PersistentVolumeClaim pod to start so the initial mount timed out. After around 5 minutes, it sorted itself out. I was able to delete the Prometheus pod and still retained data.&lt;/p></description></item><item><title>Kubectl List All Resources With Label</title><link>https://www.joshkasuboski.com/posts/list-all-resource-with-label/</link><pubDate>Sun, 24 May 2020 22:19:00 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/list-all-resource-with-label/</guid><description>&lt;p>I recently had to remove a long gone helm release that left behind a bunch of resources. This conflicted when reinstalling, so I needed to find them.&lt;/p>
&lt;h2 id="a-magic-command">A magic command&lt;/h2>
&lt;p>I'm mainly putting this here because it seemed non obvious to me.&lt;/p>
&lt;pre>&lt;code>kubectl api-resources --verbs=list -o name | xargs -n 1 kubectl get -o name -l release=prometheus-operator
&lt;/code>&lt;/pre>&lt;p>That will find all &lt;code>api-resources&lt;/code> that have the label &lt;code>release=prometheus-operator&lt;/code>. I had tried to use &lt;code>kubectl get all -A&lt;/code>, but this seemed to only return built-in types.&lt;/p></description></item><item><title>Building Multiarch Images</title><link>https://www.joshkasuboski.com/posts/build-multiarch-image/</link><pubDate>Sun, 17 May 2020 17:28:55 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/build-multiarch-image/</guid><description>&lt;p>I wanted to use &lt;a href="https://github.com/usefathom/fathom">fathom&lt;/a> on my Raspberry Pi k3s cluster, but they didn't publish a compatible ARM image. Not to be deterred I forked the repo and built my own.&lt;/p>
&lt;h2 id="how-does-one-build-for-multiple-architectures-easily">How does one build for multiple architectures easily&lt;/h2>
&lt;p>Docker has a tool called buildx. It acts as a frontend to buildkit and allows building images for multiple platforms at once.&lt;/p>
&lt;p>I followed &lt;a href="https://www.docker.com/blog/multi-arch-build-and-images-the-simple-way/">this&lt;/a> guide to create a GitHub Actions workflow to build it.&lt;/p>
&lt;h2 id="github-actions-it-up">GitHub Actions it up&lt;/h2>
&lt;p>You can skip ahead and look at my final workflow &lt;a href="https://github.com/kasuboski/fathom/blob/master/.github/workflows/docker.yml">here&lt;/a>.&lt;/p>
&lt;p>There was already a buildx action. The workflow ends up seemingly simple.&lt;/p>
&lt;ul>
&lt;li>Checkout the repo&lt;/li>
&lt;li>Set up buildx&lt;/li>
&lt;li>Login to DockerHub&lt;/li>
&lt;li>Build for linux/amd64,linux/arm/v7,linux/arm64&lt;/li>
&lt;/ul>
&lt;p>That last part includes the platforms I would care about (for now). amd64 is for your run of the mill computer, arm/v7 being what is on my Pi thanks to 32-bit Raspbian, and arm64 being a potential if I switch the OS on my Pi.&lt;/p>
&lt;p>I did have to make a change to the repo other than just adding the workflow. The build step had the &lt;code>GOARCH&lt;/code> variable set to &lt;code>amd64&lt;/code>. This caused the build to always output an &lt;code>amd64&lt;/code> binary, unsurprisingly. I realized this after two 16 minute builds&amp;hellip;&lt;/p>
&lt;p>The workflow works well, but every run so far was between 14mins and 30mins&amp;hellip; not exactly fast.&lt;/p>
&lt;h2 id="moving-forward">Moving Forward&lt;/h2>
&lt;p>There have been a number of images that aren't multi-arch that I want to run. I may look into forking and building those, but it would be nice to have a simpler solution with me not managing it.&lt;/p>
&lt;p>I could contribute to &lt;a href="https://github.com/raspbernetes/multi-arch-images">multi-arch-images&lt;/a> instead. They seem to be doing something similar but just copying the Dockerfiles needed.&lt;/p></description></item><item><title>Homelab Raspberry Pi Kubernetes Cluster Current State</title><link>https://www.joshkasuboski.com/posts/home-k8s-raspberry-update/</link><pubDate>Sun, 10 May 2020 12:20:48 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/home-k8s-raspberry-update/</guid><description>&lt;p>The cluster is alive and well and exposed to the internet ðŸ˜±&lt;/p>
&lt;p>In a previous &lt;a href="https://www.joshkasuboski.com/posts/home-k8s-raspberry-pi/">post&lt;/a>, I put out my plan for the cluster. I mostly followed along, but did end up getting &lt;a href="https://www.amazon.com/gp/product/B00A128S24">this&lt;/a> switch and &lt;a href="https://www.amazon.com/gp/product/B003L1AET2">these&lt;/a> ethernet cables.&lt;/p>
&lt;p>I ended up with this set-up which is described in more detail below.&lt;/p>
&lt;figure>&lt;a href="traffic-diagram.png">
&lt;img src="traffic-diagram.png"
alt="Traffic Diagram"/> &lt;/a>
&lt;/figure>
&lt;h2 id="hardware-and-monitoring">Hardware and Monitoring&lt;/h2>
&lt;p>The cluster is set up with Raspbian Lite on 3 Raspberry Pi 4 4gb boards. Raspbian Lite seems to not support 64bit (yet) so I may be changing the operating system. The cluster has the monitoring stack from carlosedp's &lt;a href="https://github.com/carlosedp/cluster-monitoring">repo&lt;/a>. I changed some things (mainly the ingress) in my forked &lt;a href="https://github.com/kasuboski/cluster-monitoring">repo&lt;/a>.&lt;/p>
&lt;p>This deploys some nice Grafana dashboards to see the state of the cluster as seen below. &lt;code>blueberry&lt;/code> is the master node so hovers around 768m cpu cores and 1285Mi memory usage.&lt;/p>
&lt;figure>&lt;a href="cluster-metrics.png">
&lt;img src="cluster-metrics.png"
alt="Cluster Metrics"/> &lt;/a>
&lt;/figure>
&lt;h2 id="ingress">Ingress&lt;/h2>
&lt;p>This dashboard is exposed outside the cluster using &lt;a href="https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#over-a-nodeport-service">ingress-nginx&lt;/a> with NodePorts. These nodeports are load balanced by an haproxy running in a container on my existing Raspberry Pi 3B+. This is a single point of failure for ingress ðŸ˜°, but should be good enough for now.&lt;/p>
&lt;p>The Ingress is secured using &lt;a href="https://github.com/vouch/vouch-proxy">vouch-proxy&lt;/a> with the ingress-nginx auth &lt;a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/">annotations&lt;/a>. It uses the &lt;a href="https://indieauth.net/">IndieAuth&lt;/a> backend so I can login using this site as an identity.&lt;/p>
&lt;h2 id="expose-to-the-world">Expose to the world&lt;/h2>
&lt;p>The final piece of the puzzle to expose cluster services to the internet is a Linode &lt;a href="https://www.linode.com/products/nanodes/">nanode&lt;/a>. I chose Linode largely because I had a credit&amp;hellip; It's really just providing a public ip address that can forward to my network. I didn't set up port forwarding to my network instead the haproxy Raspberry Pi and the Linode VM are running &lt;a href="https://tailscale.com/">tailscale&lt;/a>.&lt;/p>
&lt;p>Tailscale allows the nanode to connect to my Raspberry Pi with a WireGuard connection through my crazy NAT situation. The nanode is also running an haproxy on ports 80 and 443 on the public ip interface and then proxying that to the tailscale ip of the Raspberry Pi.&lt;/p>
&lt;p>I changed the DNS of &lt;code>*.joshcorp.co&lt;/code> to point to the nanode. Now that &lt;code>*.joshcorp.co&lt;/code> resolves to my cluster on the internet, I can easily use &lt;a href="https://cert-manager.io/docs/">cert-manager&lt;/a> to get a certificate from LetsEncrypt.&lt;/p>
&lt;p>After all of that, I can access &lt;code>grafana.joshcorp.co&lt;/code> outside of my network while authenticating using &lt;a href="https://www.joshkasuboski.com">www.joshkasuboski.com&lt;/a>.&lt;/p>
&lt;h2 id="whats-next">What's Next&lt;/h2>
&lt;p>I'm pretty happy with the current state. My next move will be making sure everything is tracked in git and can be reproduced. From there I can start deploying.&lt;/p></description></item><item><title>Homelab Raspberry Pi Kubernetes Cluster</title><link>https://www.joshkasuboski.com/posts/home-k8s-raspberry-pi/</link><pubDate>Sat, 02 May 2020 16:17:39 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/home-k8s-raspberry-pi/</guid><description>&lt;p>A platform to deploy my self-hosted services onto with almost $0 recurring costs.&lt;/p>
&lt;p>Previously, I was running a few things on a Raspberry Pi 3B+. Mainly, &lt;a href="https://github.com/0xERR0R/blocky">Blocky&lt;/a> as an ad-blocking local dns cache and some adventures with &lt;a href="https://www.home-assistant.io/">Home Assistant&lt;/a>. A Kubernetes cluster will enable me to easily set up new things and have proper monitoring as well.&lt;/p>
&lt;h2 id="hardware-acquisition">Hardware Acquisition&lt;/h2>
&lt;p>I bought 3 &lt;a href="https://www.canakit.com/raspberry-pi-4-4gb.html">Raspberry Pi 4 4GB&lt;/a> boards from Canakit. I also bought a pack of 5 &lt;a href="https://www.amazon.com/gp/product/B07NP96DX5">SD cards&lt;/a>. I got a &lt;a href="https://www.amazon.com/gp/product/B01NAG3V8E">6 port usb charging station&lt;/a> and USB-A to USB-C &lt;a href="https://www.amazon.com/gp/product/B01JRY0VE4">cables&lt;/a> to power them.&lt;/p>
&lt;p>I have 3 spare ethernet ports on my router so I will be using that directly for now. In the future, I may get a switch for them to connect to.&lt;/p>
&lt;h2 id="setup">Setup&lt;/h2>
&lt;p>I'm planning to install Raspian Lite and use &lt;a href="https://github.com/alexellis/k3sup">k3sup&lt;/a> to install k3s. I looked into using &lt;a href="https://github.com/rancher/k3os">k3os&lt;/a> directly, but it seems it's still not the easiest for a Raspberry Pi &lt;a href="https://github.com/rancher/k3os/issues/309">issue&lt;/a>.&lt;/p>
&lt;p>I will try to manage the cluster in a &lt;a href="https://www.weave.works/technologies/gitops/">GitOps&lt;/a> style. As such, I will disable installing the Traefik Ingress Controller and Service Load Balancer by default and instead install them separately (or choose a different option).&lt;/p>
&lt;h2 id="networking">Networking&lt;/h2>
&lt;p>I will start out with the default flannel with vxlan, but am considering the wireguard backend. I would like to be able to access some services in the cluster using something like &lt;a href="https://tailscale.com/">Tailscale&lt;/a> as well.&lt;/p>
&lt;p>This might involve running separate ingress controllers like &lt;a href="https://medium.com/@carlosedp/multiple-traefik-ingresses-with-letsencrypt-https-certificates-on-kubernetes-b590550280cf">here&lt;/a>.&lt;/p>
&lt;h2 id="monitoring">Monitoring&lt;/h2>
&lt;p>I'm going to deploy the standard &lt;code>kube-prometheus&lt;/code> set up with the Prometheus Operator using this &lt;a href="https://github.com/carlosedp/cluster-monitoring">repo&lt;/a>. It has k3s specific settings.&lt;/p></description></item><item><title>Deploying this site with GitHub Actions</title><link>https://www.joshkasuboski.com/posts/deploy-site-github-actions/</link><pubDate>Sun, 05 Apr 2020 16:18:12 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/deploy-site-github-actions/</guid><description>&lt;p>&lt;a href="https://pages.github.com/">GitHub pages&lt;/a> is a free static hosting provider that unsurprisingly works well with a git workflow. It enables &lt;code>git push&lt;/code> to deploy type workflows. This site itself is a static site built with &lt;a href="https://gohugo.io/">hugo&lt;/a> and deployed to GitHub Pages using GitHub Actions.&lt;/p>
&lt;h2 id="github-pages-repo">GitHub Pages Repo&lt;/h2>
&lt;p>GitHub Pages expects that you have a repo named something like &lt;code>user.github.io&lt;/code> that has static files you want deployed. However, if your site requires any sort of building this doesn't really work for you (at least if you want to automate the build step).&lt;/p>
&lt;p>One way around this is to treat your GitHub Pages Repo as the output for your build. You create a separate repo that just has the built static files. You can see the repo for this site at &lt;a href="https://github.com/kasuboski/kasuboski.github.io">kasuboski/kasuboski.github.io&lt;/a>.&lt;/p>
&lt;p>You'll notice it just has static html, css, etc. I write the content for this site in markdown however, using Hugo.&lt;/p>
&lt;h2 id="code-repo">Code Repo&lt;/h2>
&lt;p>The actual content is stored in a separate repo that I'm referring to as the code repo. The repo for this site is &lt;a href="https://github.com/kasuboski/personal-site">kasuboski/personal-site&lt;/a>. This repo should look a little more familiar to anyone who has used Hugo.&lt;/p>
&lt;p>The content is under &lt;code>content/posts&lt;/code> and is written in markdown. This is the repo that I actually modify and have checked out locally. If I wanted to create a new release of it manually I would build the site and push it to the GitHub Pages Repo with the commands below.&lt;/p>
&lt;pre>&lt;code>hugo --minify
cp ./public ../kasuboski.github.io/
cd ../kasuboski.github.io
git commit -am &amp;quot;cool new post&amp;quot;
git push origin master
&lt;/code>&lt;/pre>&lt;h2 id="automatic-deploy-with-github-actions">Automatic Deploy with GitHub Actions&lt;/h2>
&lt;p>I don't really want to mess with multiple repositories locally, especially when one of them is essentially machine generated. GitHub Actions can build the site with Hugo and then push it to the GitHub Pages Repo for me.&lt;/p>
&lt;p>The workflow file for this can be found &lt;a href="https://github.com/kasuboski/personal-site/blob/master/.github/workflows/gh-pages.yaml">here&lt;/a> if you just want to copy it.&lt;/p>
&lt;p>The basic steps are: check out the code repo, build the site with hugo, push the files to the GitHub Pages Repo, and notify me of the status using &lt;a href="https://pushover.net">Pushover&lt;/a>.&lt;/p>
&lt;p>It relies heavily on already available actions. The only prerequisite setup is to make a deploy key for the GitHub Pages Repo and to register an app with Pushover.&lt;/p>
&lt;p>You can find a walkthrough of creating a deploy key &lt;a href="https://github.com/peaceiris/actions-gh-pages#%EF%B8%8F-create-ssh-deploy-key">here&lt;/a>.&lt;/p>
&lt;p>Configuring a Pushover app is &lt;a href="https://pushover.net/apps/build">here&lt;/a>. You'll need the app token and user token. I added both as secrets on the Code Repo.&lt;/p>
&lt;p>Now anytime I push a change to the code repo, GitHub Actions will generate the files, update the GitHub Pages repo and send me a push notification with the status.&lt;/p></description></item><item><title>Weather Data after Dark Sky Shutdown</title><link>https://www.joshkasuboski.com/posts/dark-sky-shutdown/</link><pubDate>Sun, 05 Apr 2020 15:03:48 -0500</pubDate><guid>https://www.joshkasuboski.com/posts/dark-sky-shutdown/</guid><description>&lt;p>Dark Sky is discontinuing their API and Android app after &lt;a href="https://blog.darksky.net/dark-sky-has-a-new-home/">joining Apple&lt;/a>. It was my favorite weather app, both from a data and UX perspective. They apparently aggregate from many different sources to have the best predictions and world wide coverage. I'm not sure I actually need all of that and am curious to make my own solution.&lt;/p>
&lt;h2 id="making-my-own-app">Making my own app&lt;/h2>
&lt;p>This seems like a great opportunity to learn &lt;a href="https://flutter.dev/">Flutter&lt;/a>. It's been a while since I've done mobile development. I did both native Android and iOS as well as React Native. I'm not too keen to be building two of everything and React Native was a tooling nightmare for me.&lt;/p>
&lt;p>Flutter seems like a great solution especially with its component driven concepts. However, in order to make a weather app you actually need the forecast data.&lt;/p>
&lt;h2 id="getting-the-forecast">Getting the forecast&lt;/h2>
&lt;p>I looked at using the National Weather Service API at &lt;a href="https://www.weather.gov/documentation/services-web-api">weather.gov&lt;/a>, but it seems it only has data for a specific Kansas station at the moment. This kind of kills the project for the time being, but I hope to find the info elsewhere.&lt;/p>
&lt;p>I saw &lt;a href="https://aaronparecki.com/">Aaron Parecki&lt;/a> seems to use &lt;a href="https://www.wunderground.com/">Wunderground&lt;/a> to get current weather for &lt;a href="https://aaronparecki.com/2018/07/03/7/">his posts&lt;/a>. I'll have to see what their API is like and if there are agreeable terms.&lt;/p>
&lt;p>I believe the National Weather Service also publishes the data in a not so convenient format. I could look into downloading that periodically and exposing it myself. Maybe I could use my phone location to only download the info that I'm most likely to care about.&lt;/p></description></item><item><title>Exporting Google Saved Places</title><link>https://www.joshkasuboski.com/posts/export-google-saved-places/</link><pubDate>Sat, 25 Jan 2020 12:40:00 -0600</pubDate><guid>https://www.joshkasuboski.com/posts/export-google-saved-places/</guid><description>&lt;p>I've made use of Google Saved Places for a while now. I have the standard â­ &amp;ldquo;Starred&amp;rdquo;, ðŸš© &amp;ldquo;Want to go&amp;rdquo;, and â¤ &amp;ldquo;Favorites&amp;rdquo; lists as well as some specific to cities. Saved places is really convenient when you're deciding where to go. My want to go list helps me remember the places I want to try and the favorites list often helps me find a place if I can't quite remember the name of it.&lt;/p>
&lt;p>It's not all rosy though&amp;hellip;when searching for anything in Google Maps it inexplicably doesn't show you if the places returned are in any of your lists. They also recently made the interface better for adding places to multiple lists, but you can't search your places by city let alone type (say coffee shops I love). This could be accomplished by having many lists, however I'm not looking to manually manage a growing list of lists.&lt;/p>
&lt;h2 id="what-to-do-about-it">What to do about it&lt;/h2>
&lt;p>What I really want is a list of places with their name, address, and general categories such as restaurant or coffee shop. Then I can tag those places with things like starred, favorite, want to go, etc.&lt;/p>
&lt;p>Once I have that, I will be able to search my places by location, tag, and/or category. I'll want to be able to see them on a map similarly to how you can navigate around a city in Google Maps and see your saved places.&lt;/p>
&lt;p>I'm going to start with all of my data from Google Saved Places and then add the info I want. Since I don't plan to stop using the Google Saved Places just yet, I'll need to be able to run the import multiple times. I'll make a &lt;a href="https://en.wikipedia.org/wiki/Command-line_interface">cli&lt;/a> to help with importing the data.&lt;/p>
&lt;h2 id="getting-the-data">Getting the data&lt;/h2>
&lt;p>There doesn't seem to be an API to access your saved places. I had to resort to using &lt;a href="https://takeout.google.com">Google Takeout&lt;/a>, which if you haven't used before is quite interesting to see everything Google knows about you.&lt;/p>
&lt;p>Getting the data I wanted out of Takeout was a bit of trial and error. There are two options for Maps data as seen below, Maps and Maps (your places). Maps (your places) includes lists that aren't the starred list. Apparently, Google treats starred separately. I'm guessing this is because historically starred was the only option and was called saved. Another fun fact is that those starred places also show up at &lt;a href="https://www.google.com/bookmarks">Google Bookmarks&lt;/a>.&lt;/p>
&lt;p>&lt;img src="https://www.joshkasuboski.com/img/maps-export-products.png" alt="export products">&lt;/p>
&lt;p>If you only want your starred places from Maps you'll want to select &amp;ldquo;All Maps data included&amp;rdquo; and select only &amp;ldquo;My labeled places&amp;rdquo;. Running the export with the two maps options will get you a link to download a &lt;code>.zip&lt;/code> file. Once extracted, you'll have a variety of folders. The important files are &lt;code>Saved Places.json&lt;/code> and &lt;code>*.csv&lt;/code>.&lt;/p>
&lt;p>The Saved Places file will have your starred list in &lt;a href="https://en.wikipedia.org/wiki/GeoJSON">GeoJSON&lt;/a> format. This format provides a fair amount of info like place name, location, and general categories. The csv files are a lot less useful by themselves. For me, it was just place name and a URL. However, the URL didn't seem to be parsable so this data wasn't super useful. Still, I was able to get all of the place names that I had saved.&lt;/p>
&lt;h2 id="enhance">Enhance!&lt;/h2>
&lt;p>I want the name, address, and categories of my places. The geojson file has everything, but the csv files only provide a name. Enter the &lt;a href="https://developers.google.com/places/web-service/intro">Google Places API&lt;/a> ðŸ’¥. This API let's you lookup place data from Google, including all of the fields I want (besides user defined tags).&lt;/p>
&lt;p>Unfortunately, neither of the formats I exported from Google gave me the &lt;a href="https://developers.google.com/places/web-service/place-id">Place ID&lt;/a>, which would make it easy to find a specific place. The json file gave me a place name and address though so it's fairly simple to do a &lt;a href="https://developers.google.com/places/web-service/search">Place Search&lt;/a> using the name and address as the input. The csv files providing only the place name and obscure URL is a little more difficult.&lt;/p>
&lt;p>I ended up scraping the URL that was included for a string that looked like a Place ID. This definitely isn't perfect&amp;hellip;especially since I only look for one of at least two possible formats, but it seemed to work for my data.&lt;/p>
&lt;h2 id="next-steps">Next steps&lt;/h2>
&lt;p>So, I was able to get my saved places out of Google and tag them with the list they were on. I'm pretty much at the exact point I was at with Google Places minus the Google Maps integration. ðŸ˜’&lt;/p>
&lt;p>I want to build up an interface to add more tags easily. I also need to build the search and visualization aspects for discovering my places.&lt;/p>
&lt;p>I'd also like to set up the ability to post this info to my site using &lt;a href="https://indieweb.org/Micropub">micropub&lt;/a>, but that presumes I have micropub set up here at all.&lt;/p>
&lt;p>The code I used to parse and save my data is on GitHub. It's a &lt;a href="https://en.wikipedia.org/wiki/Command-line_interface">cli&lt;/a> called &lt;a href="https://github.com/kasuboski/neptune">neptune&lt;/a>. At this time, it let's you import and tag places and it will export each place to a json file in a folder.&lt;/p></description></item><item><title>Replacing Feedly with Microsub?</title><link>https://www.joshkasuboski.com/posts/replacing-feedly/</link><pubDate>Wed, 15 Jan 2020 18:48:10 -0600</pubDate><guid>https://www.joshkasuboski.com/posts/replacing-feedly/</guid><description>&lt;p>I use &lt;a href="https://feedly.com">Feedly&lt;/a> to manage my content subscriptions, which include a number of bigger sites and personal blogs. Feedly is nice, but I would like to be able to save and use the data from there in other ways. So, I've been looking for an open source setup that I can tweak.&lt;/p>
&lt;p>I've been trying to utilize &lt;a href="https://indieweb.org">IndieWeb&lt;/a> pieces more and more. Their &lt;a href="https://indieweb.org/why">why&lt;/a> really resonates with some of my frustrations with the current web (mainly auth and data ownership/portability). This site supports &lt;a href="https://indieauth.com/">IndieAuth&lt;/a> so I can login at supporting websites by giving my URL. The posts and contact card are also marked up with &lt;a href="http://microformats.org/">microformat&lt;/a> to be parseable.&lt;/p>
&lt;h2 id="experimenting-with-aperture-and-monocle">Experimenting with Aperture and Monocle&lt;/h2>
&lt;p>I decided to use &lt;a href="https://aperture.p3k.io/">Aperture&lt;/a> for now as my &lt;a href="https://indieweb.org/Microsub">microsub&lt;/a> server. A microsub server is responsible for fetching the content you subscribe to and making it available in a common format for a microsub reader.&lt;/p>
&lt;p>I was able to login and subscribe to &lt;a href="https://aaronparecki.com/">Aaron Parecki's&lt;/a> personal site, which immediately loaded some 1600 entries for me. After adding a &lt;code>rel=microsub&lt;/code> link to my homepage, I was able to log into &lt;a href="https://monocle.p3k.io/">Monocle&lt;/a> and view that feed in my home channel. The default view for Monocle seems to be to show everything. There is an option to only show unread, but it's not what you get by default.&lt;/p>
&lt;h2 id="moving-forward">Moving forward&lt;/h2>
&lt;p>Monocle doesn't quite fit how I want to view updates, but it did help me understand the concepts better. The free hosted Aperture only saves your data for 7 days so I probably need to either host it myself or find a different microsub server.&lt;/p>
&lt;p>I've been looking at &lt;a href="https://github.com/pstuifzand/ekster">ekster&lt;/a>. I like that it's a go binary and comes with a CLI. It has the option of importing an opml feed, which Feedly would export. It seems all of your channels and feeds are stored in a config file (that you can generate with the opml import) and redis is really meant to be a cache.&lt;/p>
&lt;p>It does seem to support the &lt;a href="https://indieweb.org/Microsub-spec#Following">follow action&lt;/a> however and it doesn't look like that updates the file. In the future, I'll probably just try to run it and see what happens.&lt;/p>
&lt;p>Ekster also has a reader associated with it, but there are a number of &lt;a href="https://indieweb.org/Microsub#Clients">others&lt;/a> to try including mobile apps.&lt;/p></description></item><item><title>Now</title><link>https://www.joshkasuboski.com/now/</link><pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.joshkasuboski.com/now/</guid><description>&lt;blockquote>
&lt;p>A now page inspired by &lt;a href="https://sivers.org/nowff">Derek Sivers&amp;rsquo;&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>My current interests are:&lt;/p>
&lt;ul>
&lt;li>Decentralized Services&lt;/li>
&lt;li>Self Hosting&lt;/li>
&lt;li>Quantified Self&lt;/li>
&lt;li>&lt;a href="https://indieweb.org/">IndieWeb&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>My Tech Setup is:&lt;/p>
&lt;ul>
&lt;li>Pixel 4a&lt;/li>
&lt;li>2020 M1 Macbook Air&lt;/li>
&lt;li>Custom Desktop (Ryzen 5 1600, 32gb, 1060)&lt;/li>
&lt;/ul>
&lt;p>You can find more on my &lt;a href="https://kit.co/kasuboski">kit.co profile&lt;/a>&lt;/p></description></item><item><title>Cheap Managed Kubernetes with Terraform</title><link>https://www.joshkasuboski.com/posts/cheap-managed-kube/</link><pubDate>Thu, 18 Apr 2019 14:15:59 -0600</pubDate><guid>https://www.joshkasuboski.com/posts/cheap-managed-kube/</guid><description>&lt;p>Kubernetes is a great way to deploy your services in a scalable and reliable way. However, it's a pretty complex system to manage yourself. Thankfully, cloud providers are offering managed versions where you only pay for the worker nodes.&lt;/p>
&lt;p>We'll use &lt;a href="https://cloud.google.com/kubernetes-engine/">GKE&lt;/a>, Google's managed kubernetes offering, to deploy a cluster so we can test out kubernetes.&lt;/p>
&lt;p>We'll use &lt;a href="https://www.terraform.io/">Terraform&lt;/a> to make sure we have a repeatable deployment process.&lt;/p>
&lt;p>If you just want to skip to the code it's on &lt;a href="https://github.com/kasuboski/cheap-managed-kubernetes">GitHub&lt;/a>.&lt;/p>
&lt;h2 id="what-well-do">What we'll do&lt;/h2>
&lt;p>The resources we'll deploy use the Google Cloud &lt;a href="https://cloud.google.com/free/">free-tier&lt;/a> extensively. If you leave it running, it should cost a little over $5 a month.&lt;/p>
&lt;p>If you're not familiar with Terraform or haven't used the Google Provider, you can get started &lt;a href="https://www.terraform.io/docs/providers/google/getting_started.html">here&lt;/a>. All of the resources it deploys will be in the free tier.&lt;/p>
&lt;p>Terraform has a concept of remote backends which allow you to save the state of your deployments (not just on your machine). This is especially helpful if you have multiple team members.&lt;/p>
&lt;p>Since we're already using Google Cloud we can use Google Cloud Storage to house our state. After changing some defaults we can run a few commands and have our cluster running.&lt;/p>
&lt;h2 id="actually-do-it">Actually do it&lt;/h2>
&lt;ul>
&lt;li>Create a Google Cloud Storage Bucket following these &lt;a href="https://cloud.google.com/storage/docs/creating-buckets">instructions&lt;/a>&lt;/li>
&lt;li>Clone the cheap-managed-kubernetes &lt;a href="https://github.com/kasuboski/cheap-managed-kubernetes">repo&lt;/a>&lt;/li>
&lt;li>Modify &lt;code>terraform.tfvars.example&lt;/code> with your gcp project and rename to &lt;code>terraform.tfvars&lt;/code>&lt;/li>
&lt;li>Modify &lt;code>backend.hcl.example&lt;/code> with the gcs bucket you created above and rename to &lt;code>backend.hcl.example&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>You should now be set up to deploy with Terraform. We'll initialize Terraform with our remote backend and run a plan. This plan will output what will be created (or destroyed). You can verify the output of the plan is correct and then run the apply.&lt;/p>
&lt;ul>
&lt;li>&lt;code>terraform init -backend-config=backend.hcl&lt;/code>&lt;/li>
&lt;li>&lt;code>terraform plan&lt;/code> This should say it will create a cluster and node pool.&lt;/li>
&lt;li>&lt;code>terraform apply&lt;/code> This will actually create the cluster and node pool.&lt;/li>
&lt;li>When you're done &lt;code>terraform destroy&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="using-your-cluster">Using your cluster&lt;/h2>
&lt;p>The output of the apply will give you the info you need to create a &lt;code>kubeconfig&lt;/code> to be able to connect to your cluster. Since we're using GKE though, I find it easier to just use the &lt;code>gcloud&lt;/code> command that will set your &lt;code>kubeconfig&lt;/code> for you.&lt;/p>
&lt;p>It should look something like &lt;code>gcloud container clusters get-credentials my-poor-gke-cluster&lt;/code> where &lt;code>my-poor-gke-cluster&lt;/code> is the name of the cluster resource in &lt;code>main.tf&lt;/code>&lt;/p>
&lt;p>Once you have your &lt;code>kubeconfig&lt;/code> set up, you can access your cluster like you normally would. Maybe try running &lt;code>kubectl get pods --all-namespaces&lt;/code>. You should see the pods that make up &lt;code>kube-system&lt;/code>.&lt;/p></description></item><item><title>About</title><link>https://www.joshkasuboski.com/about/</link><pubDate>Tue, 09 Apr 2019 00:00:00 +0000</pubDate><guid>https://www.joshkasuboski.com/about/</guid><description>&lt;h1 id="hi-there">Hi there&lt;/h1>
&lt;p>My name is Josh and I'm a software engineer. I work on all manner of things across mobile, web, and backend apps.&lt;/p>
&lt;p>Right now, my main interest is improving the developer experience and making development more accessible to take code from laptop to production.&lt;/p>
&lt;p>This involves making pipelines, eliminating boilerplate, and sending messages to make sure it's painless to deploy your code.&lt;/p>
&lt;p>You can find me on &lt;a href="https://github.com/kasuboski">Github&lt;/a> or &lt;a href="https://www.linkedin.com/in/joshkasuboski/">LinkedIn&lt;/a>&lt;/p>
&lt;p>Here is a &lt;a href="https://www.joshkasuboski.com/resume.html">resume&lt;/a>&lt;/p></description></item><item><title>Buy Me a Beer</title><link>https://www.joshkasuboski.com/payments/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.joshkasuboski.com/payments/</guid><description>&lt;p>If you liked something you saw on the site and feel like spending $5, I've got the button for you below.&lt;/p>
&lt;button id="stripe-checkout-button" class="button" role="link">Buy Me a Beer ðŸº&lt;/button>
&lt;script src="https://js.stripe.com/v3">&lt;/script>
&lt;script>
(function () {
var DOMAIN = 'https://www.joshkasuboski.com/';
var key = 'pk_live_51HJtKkASjDZJS9C9Ja9g6zTFbaTEdW7uCK2OTK1dq741RisoSBtSIac1QEMphVCEEkA7hTeuopkUsajAyWuyH7kT00cU4iVGfX';
var price = 'price_1HJtUTASjDZJS9C90kJjztUq';
var stripe = Stripe(key);
var checkoutButton = document.getElementById('stripe-checkout-button');
checkoutButton.addEventListener('click', function () {
stripe.redirectToCheckout({
lineItems: [{ price: price, quantity: 1 }],
mode: 'payment',
successUrl: DOMAIN + 'success',
cancelUrl: DOMAIN + 'canceled',
})
.then(function (result) {
if (result.error) {
var displayError = document.getElementById('error-message');
displayError.textContent = result.error.message;
}
});
});
})();
&lt;/script></description></item><item><title>Success</title><link>https://www.joshkasuboski.com/success/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.joshkasuboski.com/success/</guid><description>&lt;h2 id="thanks-for-the-contribution">Thanks for the contribution&lt;/h2></description></item><item><title>Uh Oh</title><link>https://www.joshkasuboski.com/canceled/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.joshkasuboski.com/canceled/</guid><description>&lt;h2 id="looks-like-that-didnt-work">Looks like that didn't work&lt;/h2></description></item></channel></rss>